import logging
import torch
import torchaudio
from transformers import pipeline


class WhisperWrapper:
    """Simplified Whisper ASR wrapper"""

    def __init__(self, model_id="openai/whisper-large-v3-turbo", enable_gpu_management=False):
        """
        Initialize WhisperWrapper

        Args:
            model_id: Whisper model ID, default uses openai/whisper-large-v3-turbo (faster, 50% smaller than v3)
            enable_gpu_management: Enable GPU memory management (lazy loading)
        """
        self.logger = logging.getLogger(__name__)
        self.model = None
        self.model_id = model_id
        self.enable_gpu_management = enable_gpu_management

        if not enable_gpu_management:
            # ä¼ ç»Ÿæ–¹å¼ï¼šç«‹å³åŠ è½½
            self._load_model()
        else:
            self.logger.info(f"âœ“ Whisper æ‡’åŠ è½½å·²å¯ç”¨")

    def _load_model(self):
        """åŠ è½½æ¨¡åž‹åˆ° GPU"""
        if self.model is not None:
            return
        
        try:
            self.logger.info(f"ðŸ“¥ åŠ è½½ Whisper æ¨¡åž‹åˆ° GPU...")
            self.model = pipeline("automatic-speech-recognition", model=self.model_id, device="cuda")
            self.logger.info(f"âœ“ Whisper model loaded successfully: {self.model_id} (using GPU)")
        except Exception as e:
            self.logger.error(f"âŒ Failed to load Whisper model: {e}")
            raise
    
    def offload_to_cpu(self):
        """å¸è½½æ¨¡åž‹åˆ° CPU"""
        if self.model is not None and hasattr(self.model, 'model'):
            self.logger.info("ðŸ’¾ å¸è½½ Whisper æ¨¡åž‹åˆ° CPU...")
            self.model.model = self.model.model.to('cpu')
            torch.cuda.empty_cache()
            self.logger.info("âœ… Whisper æ¨¡åž‹å·²å¸è½½åˆ° CPU")
    
    def load_to_gpu(self):
        """åŠ è½½æ¨¡åž‹åˆ° GPU"""
        if self.model is not None and hasattr(self.model, 'model'):
            self.logger.info("âš¡ æ¢å¤ Whisper æ¨¡åž‹åˆ° GPU...")
            self.model.model = self.model.model.to('cuda')
            self.logger.info("âœ… Whisper æ¨¡åž‹å·²æ¢å¤åˆ° GPU")

    def __call__(self, audio_input):
        """
        Audio to text transcription

        Args:
            audio_input: Audio file path or audio tensor

        Returns:
            Transcribed text
        """
        try:
            # æ‡’åŠ è½½
            if self.enable_gpu_management and self.model is None:
                self._load_model()
            elif self.enable_gpu_management and self.model is not None:
                self.load_to_gpu()
            
            if self.model is None:
                raise RuntimeError("Whisper model not loaded")

            # å¤„ç†éŸ³é¢‘
            result = self._transcribe(audio_input)
            
            return result
        finally:
            # å¸è½½
            if self.enable_gpu_management:
                self.offload_to_cpu()
    
    def _transcribe(self, audio_input):
        """
        æ‰§è¡Œè½¬å†™
        
        Args:
            audio_input: Audio file path or audio tensor

        Returns:
            Transcribed text
        """
        try:
            # Load audio
            if isinstance(audio_input, str):
                # Audio file path
                audio, audio_sr = torchaudio.load(audio_input)
                audio = torchaudio.functional.resample(audio, audio_sr, 16000)
                # Handle stereo to mono conversion (pipeline may not handle this)
                if audio.shape[0] > 1:
                    audio = audio.mean(dim=0, keepdim=True)  # Convert stereo to mono by averaging
                # Convert to numpy and squeeze
                audio = audio.squeeze(0).numpy()
            elif isinstance(audio_input, torch.Tensor):
                # Tensor input
                audio = audio_input.cpu()
                audio = torchaudio.functional.resample(audio, audio_sr, 16000)
                # Handle stereo to mono conversion
                if audio.ndim > 1 and audio.shape[0] > 1:
                    audio = audio.mean(dim=0, keepdim=True)
                audio = audio.squeeze().numpy()
            else:
                raise ValueError(f"Unsupported audio input type: {type(audio_input)}")

            # Transcribe
            result = self.model(audio)
            text = result.get("text", "").strip() if isinstance(result, dict) else str(result).strip()

            self.logger.debug(f"Transcription result: {text}")
            return text

        except Exception as e:
            self.logger.error(f"Audio transcription failed: {e}")
            return ""

    def is_available(self):
        """Check if whisper model is available"""
        return self.model is not None