#!/usr/bin/env python3
"""
GPU æ˜¾å­˜ç®¡ç†åŠŸèƒ½æµ‹è¯•è„šæœ¬

æµ‹è¯•å†…å®¹ï¼š
1. æ‡’åŠ è½½ï¼šé¦–æ¬¡è¯·æ±‚åŠ è½½æ¨¡å‹
2. å³ç”¨å³å¸ï¼šä»»åŠ¡å®Œæˆåè‡ªåŠ¨å¸è½½
3. å¿«é€Ÿæ¢å¤ï¼šä» CPU å¿«é€Ÿæ¢å¤åˆ° GPU
4. è‡ªåŠ¨ç›‘æ§ï¼šç©ºé—²è¶…æ—¶è‡ªåŠ¨å¸è½½
"""

import time
import torch
from gpu_manager import GPUResourceManager


def get_gpu_memory_mb():
    """è·å–å½“å‰ GPU æ˜¾å­˜å ç”¨ï¼ˆMBï¼‰"""
    if torch.cuda.is_available():
        return torch.cuda.memory_allocated() / 1024 / 1024
    return 0.0


def create_dummy_model():
    """åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿæ¨¡å‹ç”¨äºæµ‹è¯•"""
    print("ğŸ“¥ åˆ›å»ºè™šæ‹Ÿæ¨¡å‹ï¼ˆæ¨¡æ‹ŸåŠ è½½è¿‡ç¨‹ï¼‰...")
    time.sleep(2)  # æ¨¡æ‹ŸåŠ è½½æ—¶é—´
    
    # åˆ›å»ºä¸€ä¸ªå ç”¨æ˜¾å­˜çš„æ¨¡å‹
    model = torch.nn.Linear(10000, 10000).cuda()
    print(f"âœ… æ¨¡å‹åˆ›å»ºå®Œæˆï¼Œæ˜¾å­˜å ç”¨: {get_gpu_memory_mb():.1f} MB")
    return model


def test_lazy_loading():
    """æµ‹è¯•1ï¼šæ‡’åŠ è½½"""
    print("\n" + "="*60)
    print("æµ‹è¯•1ï¼šæ‡’åŠ è½½")
    print("="*60)
    
    manager = GPUResourceManager(idle_timeout=10)
    
    print(f"åˆå§‹æ˜¾å­˜: {get_gpu_memory_mb():.1f} MB")
    
    # é¦–æ¬¡è·å–æ¨¡å‹ï¼ˆåº”è¯¥è§¦å‘åŠ è½½ï¼‰
    print("\nç¬¬ä¸€æ¬¡è·å–æ¨¡å‹...")
    model = manager.get_model('test_model', create_dummy_model)
    print(f"è·å–åæ˜¾å­˜: {get_gpu_memory_mb():.1f} MB")
    
    # å†æ¬¡è·å–ï¼ˆåº”è¯¥ç›´æ¥è¿”å›ï¼‰
    print("\nç¬¬äºŒæ¬¡è·å–æ¨¡å‹ï¼ˆåº”è¯¥ç›´æ¥è¿”å›ï¼‰...")
    model = manager.get_model('test_model', create_dummy_model)
    print(f"è·å–åæ˜¾å­˜: {get_gpu_memory_mb():.1f} MB")
    
    print("\nâœ… æµ‹è¯•1é€šè¿‡ï¼šæ‡’åŠ è½½å·¥ä½œæ­£å¸¸")
    return manager


def test_force_offload(manager):
    """æµ‹è¯•2ï¼šå³ç”¨å³å¸"""
    print("\n" + "="*60)
    print("æµ‹è¯•2ï¼šå³ç”¨å³å¸")
    print("="*60)
    
    print(f"å¸è½½å‰æ˜¾å­˜: {get_gpu_memory_mb():.1f} MB")
    
    # æ‰‹åŠ¨å¸è½½
    print("\næ‰§è¡Œå¸è½½...")
    manager.force_offload('test_model')
    time.sleep(1)
    
    print(f"å¸è½½åæ˜¾å­˜: {get_gpu_memory_mb():.1f} MB")
    
    print("\nâœ… æµ‹è¯•2é€šè¿‡ï¼šå³ç”¨å³å¸å·¥ä½œæ­£å¸¸")


def test_fast_recovery(manager):
    """æµ‹è¯•3ï¼šå¿«é€Ÿæ¢å¤"""
    print("\n" + "="*60)
    print("æµ‹è¯•3ï¼šå¿«é€Ÿæ¢å¤")
    print("="*60)
    
    print(f"æ¢å¤å‰æ˜¾å­˜: {get_gpu_memory_mb():.1f} MB")
    
    # ä» CPU æ¢å¤åˆ° GPU
    print("\nä» CPU æ¢å¤æ¨¡å‹åˆ° GPU...")
    start_time = time.time()
    model = manager.get_model('test_model', create_dummy_model)
    elapsed = time.time() - start_time
    
    print(f"æ¢å¤åæ˜¾å­˜: {get_gpu_memory_mb():.1f} MB")
    print(f"æ¢å¤è€—æ—¶: {elapsed:.2f} ç§’")
    
    if elapsed < 5:
        print("\nâœ… æµ‹è¯•3é€šè¿‡ï¼šå¿«é€Ÿæ¢å¤å·¥ä½œæ­£å¸¸ï¼ˆ< 5ç§’ï¼‰")
    else:
        print(f"\nâš ï¸  æµ‹è¯•3è­¦å‘Šï¼šæ¢å¤æ—¶é—´è¾ƒé•¿ï¼ˆ{elapsed:.2f}ç§’ï¼‰")


def test_auto_monitor(manager):
    """æµ‹è¯•4ï¼šè‡ªåŠ¨ç›‘æ§"""
    print("\n" + "="*60)
    print("æµ‹è¯•4ï¼šè‡ªåŠ¨ç›‘æ§ï¼ˆç©ºé—²è¶…æ—¶ï¼‰")
    print("="*60)
    
    # å¯åŠ¨ç›‘æ§
    manager.start_monitor()
    
    # è·å–æ¨¡å‹
    print("\nè·å–æ¨¡å‹...")
    model = manager.get_model('test_model', create_dummy_model)
    print(f"è·å–åæ˜¾å­˜: {get_gpu_memory_mb():.1f} MB")
    
    # ç­‰å¾…è¶…æ—¶ï¼ˆ10ç§’ + 30ç§’æ£€æŸ¥é—´éš”ï¼‰
    print(f"\nç­‰å¾…ç©ºé—²è¶…æ—¶ï¼ˆ{manager.idle_timeout}ç§’ + 30ç§’æ£€æŸ¥é—´éš”ï¼‰...")
    for i in range(manager.idle_timeout + 35):
        time.sleep(1)
        if i % 10 == 0:
            print(f"  å·²ç­‰å¾… {i} ç§’ï¼Œå½“å‰æ˜¾å­˜: {get_gpu_memory_mb():.1f} MB")
    
    print(f"\nè¶…æ—¶åæ˜¾å­˜: {get_gpu_memory_mb():.1f} MB")
    
    # åœæ­¢ç›‘æ§
    manager.stop_monitor()
    
    print("\nâœ… æµ‹è¯•4é€šè¿‡ï¼šè‡ªåŠ¨ç›‘æ§å·¥ä½œæ­£å¸¸")


def test_status():
    """æµ‹è¯•5ï¼šçŠ¶æ€æŸ¥è¯¢"""
    print("\n" + "="*60)
    print("æµ‹è¯•5ï¼šçŠ¶æ€æŸ¥è¯¢")
    print("="*60)
    
    manager = GPUResourceManager(idle_timeout=60)
    
    # åŠ è½½æ¨¡å‹
    model = manager.get_model('test_model', create_dummy_model)
    
    # æŸ¥è¯¢çŠ¶æ€
    status = manager.get_status()
    print("\nå½“å‰çŠ¶æ€:")
    print(f"  GPU æ˜¾å­˜: {status['gpu_memory_mb']:.1f} MB")
    print(f"  ç©ºé—²è¶…æ—¶: {status['idle_timeout']} ç§’")
    print(f"  æ¨¡å‹åˆ—è¡¨:")
    for name, info in status['models'].items():
        print(f"    - {name}: {info['location']} (ç©ºé—² {info['idle_seconds']} ç§’)")
    
    print("\nâœ… æµ‹è¯•5é€šè¿‡ï¼šçŠ¶æ€æŸ¥è¯¢å·¥ä½œæ­£å¸¸")


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸ§ª GPU æ˜¾å­˜ç®¡ç†åŠŸèƒ½æµ‹è¯•")
    print("="*60)
    
    if not torch.cuda.is_available():
        print("âŒ é”™è¯¯ï¼šæœªæ£€æµ‹åˆ° CUDA è®¾å¤‡")
        return
    
    print(f"âœ… CUDA å¯ç”¨")
    print(f"   è®¾å¤‡: {torch.cuda.get_device_name(0)}")
    print(f"   åˆå§‹æ˜¾å­˜: {get_gpu_memory_mb():.1f} MB")
    
    try:
        # æµ‹è¯•1ï¼šæ‡’åŠ è½½
        manager = test_lazy_loading()
        
        # æµ‹è¯•2ï¼šå³ç”¨å³å¸
        test_force_offload(manager)
        
        # æµ‹è¯•3ï¼šå¿«é€Ÿæ¢å¤
        test_fast_recovery(manager)
        
        # æµ‹è¯•4ï¼šè‡ªåŠ¨ç›‘æ§ï¼ˆå¯é€‰ï¼Œè€—æ—¶è¾ƒé•¿ï¼‰
        # test_auto_monitor(manager)
        
        # æµ‹è¯•5ï¼šçŠ¶æ€æŸ¥è¯¢
        test_status()
        
        print("\n" + "="*60)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print("="*60)
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # æ¸…ç†
        torch.cuda.empty_cache()


if __name__ == "__main__":
    main()
