# 🚀 GPU 显存管理 - 快速开始

## 3 分钟上手

### 步骤 1：编辑启动脚本

```bash
vim start_with_gpu_management.sh
```

修改以下配置：
```bash
PROJECT_DIR="/your/project/path"  # 改为你的项目路径
PORT=7860                          # 服务端口
GPU_IDLE_TIMEOUT=600               # 空闲超时（秒）
```

### 步骤 2：启动服务

```bash
./start_with_gpu_management.sh
```

### 步骤 3：访问 UI

打开浏览器访问：http://localhost:7860

在 UI 中找到 **🎮 GPU 显存管理** 面板，查看状态。

---

## 📊 效果验证

### 1. 查看初始显存

```bash
nvidia-smi
```

应该看到显存占用很低（< 1GB）

### 2. 执行一次任务

在 UI 中：
1. 上传音频
2. 输入文本
3. 点击 CLONE

### 3. 观察显存变化

```bash
watch -n 1 nvidia-smi
```

你会看到：
- 任务开始：显存上升到 ~40GB
- 任务完成：显存下降到 ~1GB（2秒内）

### 4. 查看 GPU 状态

在 UI 的 **🎮 GPU 显存管理** 面板中：
- 点击 **🔄 刷新状态**
- 查看模型位置：应该显示 **🟡 CPU**

---

## 🎯 核心功能演示

### 功能 1：懒加载

**操作**：首次启动后，不执行任何任务

**观察**：
```bash
nvidia-smi
```
显存占用应该很低（< 1GB），因为模型还未加载

### 功能 2：即用即卸

**操作**：执行一次 CLONE 任务

**观察**：
```bash
watch -n 1 nvidia-smi
```
- 任务开始：显存 ↑ 40GB
- 任务完成：显存 ↓ 1GB（2秒内）

### 功能 3：快速恢复

**操作**：连续执行两次 CLONE 任务

**观察**：
- 第一次：加载时间 ~24秒
- 第二次：加载时间 ~26-29秒（+2-5秒）

### 功能 4：自动监控

**操作**：执行一次任务后，等待 10 分钟

**观察**：
```bash
watch -n 30 nvidia-smi
```
10 分钟后，显存应该保持在 ~1GB（已自动卸载）

### 功能 5：手动控制

**操作**：在 UI 中点击按钮

**测试**：
1. 点击 **💾 卸载到CPU** → 显存下降
2. 点击 **🔄 刷新状态** → 查看状态
3. 点击 **🗑️ 完全释放** → 显存归零

---

## 🎮 UI 面板说明

### GPU 状态显示

```
🎮 GPU 显存管理状态
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
GPU 显存占用: 1024.5 MB
空闲超时: 600 秒

📦 模型状态:
  • tts: 🟡 CPU
    空闲时间: 45 秒
```

### 状态图标含义

- 🟢 GPU：模型在 GPU 上（正在使用）
- 🟡 CPU：模型在 CPU 上（已卸载，可快速恢复）
- ⚪ 未加载：模型未加载（首次使用需要加载）

### 控制按钮

- **🔄 刷新状态**：查看最新状态
- **💾 卸载到CPU**：立即释放 GPU 显存
- **🗑️ 完全释放**：清空所有缓存

---

## 📈 性能对比

### 显存占用

| 时刻 | 传统方式 | GPU 管理 |
|------|---------|---------|
| 启动后 | 40 GB | 0 GB |
| 首次任务 | 40 GB | 40 GB |
| 任务完成 | 40 GB | 1 GB ✅ |
| 空闲 10 分钟 | 40 GB | 1 GB ✅ |

### 响应时间

| 场景 | 传统方式 | GPU 管理 |
|------|---------|---------|
| 首次请求 | 24s | 24s |
| 第二次请求 | 24s | 26-29s |
| 缓存命中 | 8s | 10-13s |

---

## 🔧 常见问题

### Q1：GPU 管理未生效？

**检查**：
```bash
docker logs step-audio-gpu-managed | grep "GPU 管理"
```

应该看到：`🚀 GPU 管理已启用`

### Q2：显存没有释放？

**检查**：
1. 确认任务已完成
2. 等待 2-3 秒
3. 运行 `nvidia-smi` 查看

### Q3：恢复速度很慢？

**可能原因**：
- CPU 内存不足
- 系统负载过高

**解决**：
```bash
# 检查内存
free -h

# 检查负载
top
```

---

## 💡 使用建议

### 推荐配置

| 使用频率 | 超时时间 | 说明 |
|---------|---------|------|
| 高频（>10次/分钟） | 不启用 | 使用传统方式 |
| 中频（1-10次/分钟） | 600秒 | 平衡性能和显存 |
| 低频（<1次/分钟） | 1800秒 | 最大化显存释放 |

### 多服务共享

```bash
# 服务 A（端口 7860）
./start_with_gpu_management.sh  # GPU_IDLE_TIMEOUT=300

# 服务 B（端口 7861）
# 修改 PORT=7861，然后启动
./start_with_gpu_management.sh
```

---

## 📚 更多信息

- [完整指南](GPU_MANAGEMENT.md) - 详细的使用说明
- [功能总结](GPU_MANAGEMENT_SUMMARY.md) - 技术实现细节
- [主文档](README.md) - 项目完整文档

---

## 🎉 开始使用

现在你已经了解了基本用法，可以开始使用 GPU 显存管理功能了！

**记住**：
- ✅ 空闲时节省 97.5% 显存
- ✅ 使用时快速恢复（+2-5秒）
- ✅ 自动监控，无需手动管理

**祝使用愉快！** 🚀
