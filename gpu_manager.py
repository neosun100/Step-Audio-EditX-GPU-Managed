"""
GPU Resource Manager - æ™ºèƒ½æ˜¾å­˜ç®¡ç†ï¼ˆæ‡’åŠ è½½ + å³ç”¨å³å¸ï¼‰

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. æ‡’åŠ è½½ï¼šé¦–æ¬¡è¯·æ±‚æ—¶åŠ è½½æ¨¡å‹åˆ° GPU
2. å³ç”¨å³å¸ï¼šä»»åŠ¡å®Œæˆåç«‹å³è½¬ç§»åˆ° CPUï¼Œé‡Šæ”¾æ˜¾å­˜
3. å¿«é€Ÿæ¢å¤ï¼šä» CPU å¿«é€Ÿè½¬ç§»å› GPUï¼ˆ2-5ç§’ï¼‰
4. è‡ªåŠ¨ç›‘æ§ï¼šç©ºé—²è¶…æ—¶è‡ªåŠ¨é‡Šæ”¾

çŠ¶æ€è½¬æ¢ï¼š
æœªåŠ è½½ â”€â”€é¦–æ¬¡(20-30s)â”€â”€â†’ GPU â”€â”€ä»»åŠ¡å®Œæˆ(2s)â”€â”€â†’ CPU â”€â”€æ–°è¯·æ±‚(2-5s)â”€â”€â†’ GPU
  â†‘                                                    â†“
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€è¶…æ—¶/æ‰‹åŠ¨é‡Šæ”¾(1s)â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
"""

import torch
import threading
import time
import logging
from typing import Optional, Callable, Any
import gc

logger = logging.getLogger(__name__)


class GPUResourceManager:
    """GPU èµ„æºç®¡ç†å™¨"""
    
    def __init__(self, idle_timeout: int = 600):
        """
        åˆå§‹åŒ– GPU èµ„æºç®¡ç†å™¨
        
        Args:
            idle_timeout: ç©ºé—²è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 10 åˆ†é’Ÿ
        """
        self.idle_timeout = idle_timeout
        self.lock = threading.Lock()
        self.running = False
        self.monitor_thread = None
        
        # æ¨¡å‹çŠ¶æ€
        self.models = {}  # {model_name: model_instance}
        self.models_cpu = {}  # {model_name: cpu_cached_model}
        self.last_use_time = {}  # {model_name: timestamp}
        self.model_locations = {}  # {model_name: 'gpu'/'cpu'/'unloaded'}
        
        logger.info(f"ğŸš€ GPU èµ„æºç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ (è¶…æ—¶: {idle_timeout}ç§’)")
    
    def get_model(self, model_name: str, load_func: Callable[[], Any]) -> Any:
        """
        è·å–æ¨¡å‹ï¼ˆæ‡’åŠ è½½é€»è¾‘ï¼‰
        
        Args:
            model_name: æ¨¡å‹åç§°ï¼ˆå¦‚ 'llm', 'vocoder'ï¼‰
            load_func: æ¨¡å‹åŠ è½½å‡½æ•°
            
        Returns:
            æ¨¡å‹å®ä¾‹ï¼ˆåœ¨ GPU ä¸Šï¼‰
        """
        with self.lock:
            self.last_use_time[model_name] = time.time()
            
            # æƒ…å†µ1: æ¨¡å‹å·²åœ¨ GPU ä¸Š
            if model_name in self.models and self.models[model_name] is not None:
                logger.debug(f"âœ… æ¨¡å‹ {model_name} å·²åœ¨ GPU ä¸Š")
                return self.models[model_name]
            
            # æƒ…å†µ2: æ¨¡å‹åœ¨ CPU ç¼“å­˜ä¸­ï¼Œå¿«é€Ÿè½¬ç§»åˆ° GPU
            # æ³¨æ„ï¼šå¯¹äºå¤åˆå¯¹è±¡ï¼ˆå¦‚ StepAudioTTSï¼‰ï¼Œä¸è¿›è¡Œ CPU/GPU è½¬ç§»
            # ç›´æ¥è¿”å›ç¼“å­˜çš„å¯¹è±¡
            if model_name in self.models_cpu and self.models_cpu[model_name] is not None:
                logger.info(f"âš¡ ä»ç¼“å­˜æ¢å¤æ¨¡å‹ {model_name}...")
                start_time = time.time()
                
                model = self.models_cpu[model_name]
                self.models[model_name] = model
                self.models_cpu[model_name] = None
                self.model_locations[model_name] = 'gpu'
                
                elapsed = time.time() - start_time
                logger.info(f"âœ… æ¨¡å‹ {model_name} æ¢å¤å®Œæˆ ({elapsed:.2f}ç§’)")
                return model
            
            # æƒ…å†µ3: é¦–æ¬¡åŠ è½½ï¼Œä»ç£ç›˜åŠ è½½åˆ° GPU
            logger.info(f"ğŸ“¥ é¦–æ¬¡åŠ è½½æ¨¡å‹ {model_name} åˆ° GPU...")
            start_time = time.time()
            
            model = load_func()
            self.models[model_name] = model
            self.model_locations[model_name] = 'gpu'
            
            elapsed = time.time() - start_time
            logger.info(f"âœ… æ¨¡å‹ {model_name} åŠ è½½å®Œæˆ ({elapsed:.2f}ç§’)")
            return model
    
    def force_offload(self, model_name: Optional[str] = None):
        """
        å³ç”¨å³å¸ï¼šå°†æ¨¡å‹ä» GPU è½¬ç§»åˆ° CPU
        
        Args:
            model_name: æ¨¡å‹åç§°ï¼ŒNone è¡¨ç¤ºå¸è½½æ‰€æœ‰æ¨¡å‹
        """
        with self.lock:
            if model_name:
                self._offload_single_model(model_name)
            else:
                # å¸è½½æ‰€æœ‰æ¨¡å‹
                for name in list(self.models.keys()):
                    self._offload_single_model(name)
    
    def _offload_single_model(self, model_name: str):
        """å¸è½½å•ä¸ªæ¨¡å‹åˆ° CPU"""
        if model_name not in self.models or self.models[model_name] is None:
            return
        
        logger.info(f"ğŸ’¾ å¸è½½æ¨¡å‹ {model_name} åˆ° CPU...")
        start_time = time.time()
        
        model = self.models[model_name]
        # å¯¹äºå¤åˆå¯¹è±¡ï¼ˆå¦‚ StepAudioTTSï¼‰ï¼Œåªæ˜¯ç§»åŠ¨å¼•ç”¨ï¼Œä¸è°ƒç”¨ .to()
        self.models_cpu[model_name] = model
        self.models[model_name] = None
        self.model_locations[model_name] = 'cpu'
        
        # æ¸…ç† GPU ç¼“å­˜
        torch.cuda.empty_cache()
        gc.collect()
        
        elapsed = time.time() - start_time
        gpu_mem = self._get_gpu_memory_mb()
        logger.info(f"âœ… æ¨¡å‹ {model_name} å·²å¸è½½ ({elapsed:.2f}ç§’, GPUæ˜¾å­˜: {gpu_mem:.0f}MB)")
    
    def force_release(self, model_name: Optional[str] = None):
        """
        å®Œå…¨é‡Šæ”¾ï¼šæ¸…ç©º GPU å’Œ CPU ç¼“å­˜
        
        Args:
            model_name: æ¨¡å‹åç§°ï¼ŒNone è¡¨ç¤ºé‡Šæ”¾æ‰€æœ‰æ¨¡å‹
        """
        with self.lock:
            if model_name:
                self._release_single_model(model_name)
            else:
                # é‡Šæ”¾æ‰€æœ‰æ¨¡å‹
                for name in list(self.models.keys()):
                    self._release_single_model(name)
    
    def _release_single_model(self, model_name: str):
        """å®Œå…¨é‡Šæ”¾å•ä¸ªæ¨¡å‹"""
        logger.info(f"ğŸ—‘ï¸  å®Œå…¨é‡Šæ”¾æ¨¡å‹ {model_name}...")
        
        self.models[model_name] = None
        self.models_cpu[model_name] = None
        if model_name in self.model_locations:
            del self.model_locations[model_name]
        if model_name in self.last_use_time:
            del self.last_use_time[model_name]
        
        torch.cuda.empty_cache()
        gc.collect()
        
        gpu_mem = self._get_gpu_memory_mb()
        logger.info(f"âœ… æ¨¡å‹ {model_name} å·²å®Œå…¨é‡Šæ”¾ (GPUæ˜¾å­˜: {gpu_mem:.0f}MB)")
    
    def start_monitor(self):
        """å¯åŠ¨ç›‘æ§çº¿ç¨‹"""
        if self.running:
            logger.warning("ç›‘æ§çº¿ç¨‹å·²åœ¨è¿è¡Œ")
            return
        
        self.running = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)
        self.monitor_thread.start()
        logger.info("ğŸ” GPU ç›‘æ§çº¿ç¨‹å·²å¯åŠ¨")
    
    def stop_monitor(self):
        """åœæ­¢ç›‘æ§çº¿ç¨‹"""
        self.running = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=5)
        logger.info("â¹ï¸  GPU ç›‘æ§çº¿ç¨‹å·²åœæ­¢")
    
    def _monitor_loop(self):
        """ç›‘æ§å¾ªç¯ï¼šæ£€æŸ¥ç©ºé—²è¶…æ—¶"""
        while self.running:
            time.sleep(30)  # æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡
            
            with self.lock:
                current_time = time.time()
                
                for model_name in list(self.models.keys()):
                    if model_name not in self.last_use_time:
                        continue
                    
                    idle_time = current_time - self.last_use_time[model_name]
                    
                    # è¶…æ—¶è‡ªåŠ¨å¸è½½åˆ° CPU
                    if idle_time > self.idle_timeout:
                        if self.models.get(model_name) is not None:
                            logger.info(f"â° æ¨¡å‹ {model_name} ç©ºé—² {idle_time:.0f}ç§’ï¼Œè‡ªåŠ¨å¸è½½")
                            self._offload_single_model(model_name)
    
    def get_status(self) -> dict:
        """è·å–å½“å‰çŠ¶æ€"""
        with self.lock:
            status = {
                'models': {},
                'gpu_memory_mb': self._get_gpu_memory_mb(),
                'idle_timeout': self.idle_timeout
            }
            
            for model_name in self.model_locations:
                location = self.model_locations[model_name]
                idle_time = time.time() - self.last_use_time.get(model_name, time.time())
                
                status['models'][model_name] = {
                    'location': location,
                    'idle_seconds': int(idle_time)
                }
            
            return status
    
    def _get_gpu_memory_mb(self) -> float:
        """è·å–å½“å‰ GPU æ˜¾å­˜å ç”¨ï¼ˆMBï¼‰"""
        if torch.cuda.is_available():
            return torch.cuda.memory_allocated() / 1024 / 1024
        return 0.0
    
    def update_timeout(self, new_timeout: int):
        """æ›´æ–°ç©ºé—²è¶…æ—¶æ—¶é—´"""
        with self.lock:
            self.idle_timeout = new_timeout
            logger.info(f"âš™ï¸  ç©ºé—²è¶…æ—¶å·²æ›´æ–°ä¸º {new_timeout} ç§’")


# å…¨å±€å•ä¾‹
_global_gpu_manager: Optional[GPUResourceManager] = None


def get_gpu_manager(idle_timeout: int = 600) -> GPUResourceManager:
    """è·å–å…¨å±€ GPU ç®¡ç†å™¨å•ä¾‹"""
    global _global_gpu_manager
    if _global_gpu_manager is None:
        _global_gpu_manager = GPUResourceManager(idle_timeout=idle_timeout)
        _global_gpu_manager.start_monitor()
    return _global_gpu_manager
