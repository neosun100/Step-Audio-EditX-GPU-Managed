import gradio as gr
import os
import argparse
import torch
import logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
from datetime import datetime
import time
import torchaudio
import librosa
import soundfile as sf

# Project imports
from tokenizer import StepAudioTokenizer
from tts import StepAudioTTS
from tts_gpu_managed import GPUManagedTTS
from model_loader import ModelSource
from config.edit_config import get_supported_edit_types, get_edit_type_key, get_edit_info_key
from whisper_wrapper import WhisperWrapper
from gpu_manager import get_gpu_manager

# Configure logging
logger = logging.getLogger(__name__)

# Save audio to temporary directory
def save_audio(audio_type, audio_data, sr, tmp_dir):
    """Save audio data to a temporary file with timestamp"""
    current_time = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    save_path = os.path.join(tmp_dir, audio_type, f"{current_time}.wav")
    os.makedirs(os.path.dirname(save_path), exist_ok=True)

    try:
        if isinstance(audio_data, torch.Tensor):
            torchaudio.save(save_path, audio_data, sr)
        else:
            sf.write(save_path, audio_data, sr)
        logger.debug(f"Audio saved to: {save_path}")
        return save_path
    except Exception as e:
        logger.error(f"Failed to save audio: {e}")
        raise


class EditxTab:
    """Audio editing and voice cloning interface tab"""

    def __init__(self, args, encoder=None, tts_engine=None):
        self.args = args
        self.encoder = encoder  # Store encoder for cache stats
        self.tts_engine = tts_engine  # Store TTS engine for GPU management
        self.edit_type_list = list(get_supported_edit_types().keys())
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        self.enable_auto_transcribe = getattr(args, 'enable_auto_transcribe', False)
        self.live_logs = []  # Store live execution logs
        self.max_logs = 100  # Maximum number of logs to keep

    def history_messages_to_show(self, messages):
        """Convert message history to gradio chatbot format"""
        show_msgs = []
        for message in messages:
            edit_type = message['edit_type']
            edit_info = message['edit_info']
            source_text = message['source_text']
            target_text = message['target_text']
            raw_audio_part = message['raw_wave']
            edit_audio_part = message['edit_wave']
            type_str = f"{edit_type}-{edit_info}" if edit_info is not None else f"{edit_type}"
            show_msgs.extend([
                {"role": "user", "content": f"ä»»åŠ¡ç±»å‹ï¼š{type_str}\næ–‡æœ¬ï¼š{source_text}"},
                {"role": "user", "content": gr.Audio(value=raw_audio_part, interactive=False)},
                {"role": "assistant", "content": f"è¾“å‡ºéŸ³é¢‘ï¼š\næ–‡æœ¬ï¼š{target_text}"},
                {"role": "assistant", "content": gr.Audio(value=edit_audio_part, interactive=False)}
            ])
        return show_msgs

    def generate_clone(self, prompt_text_input, prompt_audio_input, generated_text, edit_type, edit_info, model_variant, intensity, state):
        """Generate cloned audio"""
        self.add_log("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        self.add_log("ğŸ¤ å¼€å§‹ CLONE æ“ä½œ")
        self.add_log(f"   æ¨¡å‹: {model_variant} | å¼ºåº¦: {intensity}")
        self.logger.info("Starting voice cloning process")
        self.logger.info(f"   Model: {model_variant}, Intensity: {intensity}")
        state['history_audio'] = []
        state['history_messages'] = []

        # Input validation
        if not prompt_text_input or prompt_text_input.strip() == "":
            error_msg = "[Error] Uploaded text cannot be empty."
            self.logger.error(error_msg)
            self.add_log(f"âŒ {error_msg}")
            return [{"role": "user", "content": error_msg}], state, "", self.get_live_logs()
        if not prompt_audio_input:
            error_msg = "[Error] Uploaded audio cannot be empty."
            self.logger.error(error_msg)
            self.add_log(f"âŒ {error_msg}")
            return [{"role": "user", "content": error_msg}], state, "", self.get_live_logs()
        if not generated_text or generated_text.strip() == "":
            error_msg = "[Error] Clone content cannot be empty."
            self.logger.error(error_msg)
            self.add_log(f"âŒ {error_msg}")
            return [{"role": "user", "content": error_msg}], state, "", self.get_live_logs()
        actual_type = get_edit_type_key(edit_type)
        if actual_type not in {"clone", "clone_with_emotion", "clone_with_style"}:
            error_msg = "[Error] CLONE button must use clone task."
            self.logger.error(error_msg)
            self.add_log(f"âŒ {error_msg}")
            return [{"role": "user", "content": error_msg}], state, "", self.get_live_logs()

        try:
            # Use common_tts_engine for cloning
            self.add_log("ğŸ“¥ è¾“å…¥éªŒè¯é€šè¿‡ï¼Œå¼€å§‹å…‹éš†...")
            self.add_log(f"ğŸ” ä»»åŠ¡ç±»å‹: {edit_type} -> {actual_type}")
            self.add_log(f"ğŸ” å­ä»»åŠ¡: {edit_info} -> {actual_edit_info if edit_info else 'None'}")
            clone_start = time.time()
            
            # Check if this is a two-step operation
            actual_edit_info = get_edit_info_key(edit_info) if edit_info else None
            if actual_type in {"clone_with_emotion", "clone_with_style"}:
                # Step 1: Clone with new text
                self.add_log(f"ğŸ”„ Step 1/2: å…‹éš†æ–°æ–‡æœ¬...")
                output_audio, output_sr = common_tts_engine.clone(
                    prompt_audio_input, prompt_text_input, generated_text
                )
                
                # Save cloned audio to temp file
                if isinstance(output_audio, torch.Tensor):
                    cloned_numpy = output_audio.cpu().numpy().squeeze()
                else:
                    cloned_numpy = output_audio
                temp_cloned_path = save_audio("cloned_temp", cloned_numpy, output_sr, self.args.tmp_dir)
                
                # Step 2: Apply emotion or style
                edit_type_for_step2 = "emotion" if "emotion" in actual_type else "style"
                self.add_log(f"ğŸ¨ Step 2/2: åº”ç”¨{edit_type_for_step2} ({actual_edit_info})...")
                output_audio, output_sr = common_tts_engine.edit(
                    temp_cloned_path, generated_text, edit_type_for_step2, actual_edit_info, generated_text
                )
            else:
                # Normal clone
                output_audio, output_sr = common_tts_engine.clone(
                    prompt_audio_input, prompt_text_input, generated_text
                )
            
            clone_time = time.time() - clone_start
            self.add_log(f"âœ… å…‹éš†å®Œæˆï¼Œè€—æ—¶: {clone_time:.2f}s")

            if output_audio is not None and output_sr is not None:
                # Convert tensor to numpy if needed
                if isinstance(output_audio, torch.Tensor):
                    audio_numpy = output_audio.cpu().numpy().squeeze()
                else:
                    audio_numpy = output_audio

                # Load original audio for comparison
                input_audio_data_numpy, input_sample_rate = librosa.load(prompt_audio_input)

                # Create message for history
                cur_assistant_msg = {
                    "edit_type": edit_type,
                    "edit_info": edit_info,
                    "source_text": prompt_text_input,
                    "target_text": generated_text,
                    "raw_wave": (input_sample_rate, input_audio_data_numpy),
                    "edit_wave": (output_sr, audio_numpy),
                }
                state["history_audio"].append((output_sr, audio_numpy, generated_text))
                state["history_messages"].append(cur_assistant_msg)

                show_msgs = self.history_messages_to_show(state["history_messages"])
                
                # è‡ªåŠ¨æ›´æ–°ç¼“å­˜ç»Ÿè®¡
                cache_stats_text = self.format_cache_stats()
                self.logger.info("Voice cloning completed successfully")
                self.add_log("ğŸ‰ æ“ä½œæˆåŠŸå®Œæˆï¼")
                return show_msgs, state, cache_stats_text, self.get_live_logs()
            else:
                error_msg = "[Error] Clone failed"
                self.logger.error(error_msg)
                self.add_log(f"âŒ {error_msg}")
                return [{"role": "user", "content": error_msg}], state, "", self.get_live_logs()

        except Exception as e:
            error_msg = f"[Error] Clone failed: {str(e)}"
            self.logger.error(error_msg)
            self.add_log(f"âŒ å¼‚å¸¸: {str(e)}")
            cache_stats_text = self.format_cache_stats()
            return [{"role": "user", "content": error_msg}], state, cache_stats_text, self.get_live_logs()
        
    def generate_edit(self, prompt_text_input, prompt_audio_input, generated_text, edit_type, edit_info, model_variant, intensity, state):
        """Generate edited audio"""
        self.logger.info(f"   Model: {model_variant}, Intensity: {intensity}")
        self.logger.info("Starting audio editing process")

        # Input validation
        if not prompt_audio_input:
            error_msg = "[Error] Uploaded audio cannot be empty."
            self.logger.error(error_msg)
            return [{"role": "user", "content": error_msg}], state

        try:
            # Determine which audio to use
            if len(state["history_audio"]) == 0:
                # First edit - use uploaded audio
                audio_to_edit = prompt_audio_input
                text_to_use = prompt_text_input
                self.logger.debug("Using prompt audio, no history found")
            else:
                # Use previous edited audio - save it to temp file first
                sample_rate, audio_numpy, previous_text = state["history_audio"][-1]
                temp_path = save_audio("temp", audio_numpy, sample_rate, self.args.tmp_dir)
                audio_to_edit = temp_path
                text_to_use = previous_text
                self.logger.debug(f"Using previous audio from history, count: {len(state['history_audio'])}")

            # æå–å®é™…çš„ç¼–è¾‘ç±»å‹å’Œä¿¡æ¯é”®
            actual_edit_type = get_edit_type_key(edit_type)
            actual_edit_info = get_edit_info_key(edit_info) if edit_info else None
            
            # Handle clone_with_emotion and clone_with_style (two-step process)
            if actual_edit_type in {"clone_with_emotion", "clone_with_style"}:
                # Step 1: Clone with new text
                self.add_log(f"ğŸ”„ Step 1/2: Cloning with new text...")
                cloned_audio, cloned_sr = common_tts_engine.clone(
                    audio_to_edit, text_to_use, generated_text
                )
                
                # Save cloned audio to temp file
                if isinstance(cloned_audio, torch.Tensor):
                    cloned_numpy = cloned_audio.cpu().numpy().squeeze()
                else:
                    cloned_numpy = cloned_audio
                temp_cloned_path = save_audio("cloned_temp", cloned_numpy, cloned_sr, self.args.tmp_dir)
                
                # Step 2: Apply emotion or style
                self.add_log(f"ğŸ¨ Step 2/2: Applying {actual_edit_type.split('_')[-1]}...")
                edit_type_for_step2 = "emotion" if "emotion" in actual_edit_type else "style"
                output_audio, output_sr = common_tts_engine.edit(
                    temp_cloned_path, generated_text, edit_type_for_step2, actual_edit_info, generated_text
                )
            # For para-linguistic, use generated_text; otherwise use source text
            elif actual_edit_type not in {"paralinguistic"}:
                generated_text = text_to_use
                output_audio, output_sr = common_tts_engine.edit(
                    audio_to_edit, text_to_use, actual_edit_type, actual_edit_info, generated_text
                )
            else:
                # paralinguistic case
                output_audio, output_sr = common_tts_engine.edit(
                    audio_to_edit, text_to_use, actual_edit_type, actual_edit_info, generated_text
                )

            if output_audio is not None and output_sr is not None:
                # Convert tensor to numpy if needed
                if isinstance(output_audio, torch.Tensor):
                    audio_numpy = output_audio.cpu().numpy().squeeze()
                else:
                    audio_numpy = output_audio

                # Load original audio for comparison
                if len(state["history_audio"]) == 0:
                    input_audio_data_numpy, input_sample_rate = librosa.load(prompt_audio_input)
                else:
                    input_sample_rate, input_audio_data_numpy, _ = state["history_audio"][-1]

                # Create message for history
                cur_assistant_msg = {
                    "edit_type": edit_type,
                    "edit_info": edit_info,
                    "source_text": text_to_use,
                    "target_text": generated_text,
                    "raw_wave": (input_sample_rate, input_audio_data_numpy),
                    "edit_wave": (output_sr, audio_numpy),
                }
                state["history_audio"].append((output_sr, audio_numpy, generated_text))
                state["history_messages"].append(cur_assistant_msg)

                show_msgs = self.history_messages_to_show(state["history_messages"])
                self.logger.info("Audio editing completed successfully")
                return show_msgs, state
            else:
                error_msg = "[Error] Edit failed"
                self.logger.error(error_msg)
                return [{"role": "user", "content": error_msg}], state

        except Exception as e:
            error_msg = f"[Error] Edit failed: {str(e)}"
            self.logger.error(error_msg)
            return [{"role": "user", "content": error_msg}], state

    def clear_history(self, state):
        """Clear conversation history"""
        state["history_messages"] = []
        state["history_audio"] = []
        return [], state

    def init_state(self):
        """Initialize conversation state"""
        return {
            "history_messages": [],
            "history_audio": []
        }

    def register_components(self):
        """Register gradio components - maintaining exact layout from original"""
        with gr.Tab("Editx"):
            with gr.Row():
                with gr.Column():
                    self.model_input = gr.Textbox(label="Model Name", value="Step-Audio-EditX", scale=1)
                    self.prompt_text_input = gr.Textbox(label="Prompt Text", value="", scale=1)
                    self.prompt_audio_input = gr.Audio(
                        sources=["upload", "microphone"],
                        format="wav",
                        type="filepath",
                        label="Input Audio",
                    )
                    self.generated_text = gr.Textbox(label="Target Text", lines=1, max_lines=200, max_length=1000)
                    
                    # Model Variant Selection
                    self.model_variant = gr.Radio(
                        label="ğŸ¯ Model Variant",
                        choices=["base", "awq", "bnb"],
                        value="base",
                        info="base: åŸå§‹æ¨¡å‹ | awq: AWQ 4-bit | bnb: BnB 4-bit"
                    )
                    
                    # Intensity Slider
                    self.intensity = gr.Slider(
                        label="ğŸšï¸ Effect Intensity (å¼ºåº¦)",
                        minimum=0.1,
                        maximum=3.0,
                        value=1.0,
                        step=0.1,
                        info="è°ƒæ•´æ•ˆæœå¼ºåº¦ (0.1=æœ€å¼±, 1.0=æ ‡å‡†, 3.0=æœ€å¼º)"
                    )
                    
                    # FunASR Cache Stats
                    with gr.Accordion("ğŸ“Š FunASR ç¼“å­˜ç»Ÿè®¡", open=True):
                        self.cache_stats_display = gr.Textbox(
                            label="ç¼“å­˜æ€§èƒ½",
                            value="ç­‰å¾…æ•°æ®...\nç‚¹å‡» CLONE æŒ‰é’®åè‡ªåŠ¨æ›´æ–°",
                            lines=8,
                            max_lines=10,
                            interactive=False,
                            show_copy_button=True
                        )
                        with gr.Row():
                            self.refresh_cache_btn = gr.Button("ğŸ”„ åˆ·æ–°ç»Ÿè®¡", size="sm")
                            self.clear_cache_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºç¼“å­˜", size="sm")
                    
                    # GPU Management (if enabled)
                    if self.args.enable_gpu_management:
                        with gr.Accordion("ğŸ® GPU æ˜¾å­˜ç®¡ç†", open=True):
                            self.gpu_status_display = gr.Textbox(
                                label="GPU çŠ¶æ€",
                                value="ç­‰å¾…æŸ¥è¯¢...",
                                lines=6,
                                max_lines=10,
                                interactive=False,
                                show_copy_button=True
                            )
                            with gr.Row():
                                self.refresh_gpu_btn = gr.Button("ğŸ”„ åˆ·æ–°çŠ¶æ€", size="sm")
                                self.offload_gpu_btn = gr.Button("ğŸ’¾ å¸è½½åˆ°CPU", size="sm")
                                self.release_gpu_btn = gr.Button("ğŸ—‘ï¸ å®Œå…¨é‡Šæ”¾", size="sm")
                    
                with gr.Column():
                    with gr.Row():
                        self.edit_type = gr.Dropdown(label="Task (ä»»åŠ¡)", choices=self.edit_type_list, value="clone (å…‹éš†)")
                        self.edit_info = gr.Dropdown(label="Sub-task (å­ä»»åŠ¡)", choices=[], value=None)
                    self.chat_box = gr.Chatbot(label="History (å†å²è®°å½•)", type="messages", height=480*1)
                    
                    # ğŸ”¥ å®æ—¶æ—¥å¿—æ˜¾ç¤ºåŒºåŸŸ
                    with gr.Accordion("ğŸ“‹ å®æ—¶è¿è¡Œæ—¥å¿—", open=True):
                        self.live_log_display = gr.Textbox(
                            label="æ‰§è¡Œæ—¥å¿— (å¸¦æ—¶é—´æˆ³)",
                            value="ç­‰å¾…æ‰§è¡Œ...\næ—¥å¿—å°†åœ¨ CLONE/EDIT æ“ä½œæ—¶è‡ªåŠ¨æ›´æ–°",
                            lines=12,
                            max_lines=20,
                            interactive=False,
                            show_copy_button=True,
                            autoscroll=True
                        )
                        with gr.Row():
                            self.refresh_log_btn = gr.Button("ğŸ”„ åˆ·æ–°æ—¥å¿—", size="sm")
                            self.clear_log_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºæ—¥å¿—", size="sm")
            with gr.Row():
                with gr.Column():
                    with gr.Row():
                        self.button_tts = gr.Button("CLONE", variant="primary")
                        self.button_edit = gr.Button("EDIT", variant="primary")
                with gr.Column():
                    self.clean_history_submit = gr.Button("Clear History", variant="primary")

            gr.Markdown("---")
            
            # åŠŸèƒ½è¯´æ˜åŒºåŸŸ
            with gr.Accordion("ğŸ“– åŠŸèƒ½è¯´æ˜ä¸ä½¿ç”¨æŒ‡å—", open=False):
                gr.Markdown("""
                ## ğŸ¯ æŒ‰é’®è¯´æ˜
                
                - **CLONEï¼ˆå…‹éš†ï¼‰**: åŸºäºä¸Šä¼ çš„å‚è€ƒéŸ³é¢‘å’Œæ–‡æœ¬ï¼Œåˆæˆæ–°çš„éŸ³é¢‘ã€‚ä»…ç”¨äºå…‹éš†æ¨¡å¼ï¼Œä½¿ç”¨æ—¶ä¼šæ¸…ç©ºå†å²è®°å½•ã€‚
                - **EDITï¼ˆç¼–è¾‘ï¼‰**: åŸºäºä¸Šä¼ çš„éŸ³é¢‘è¿›è¡Œç¼–è¾‘ï¼Œæˆ–åœ¨ä¸Šä¸€è½®ç”Ÿæˆçš„éŸ³é¢‘åŸºç¡€ä¸Šç»§ç»­å åŠ ç¼–è¾‘æ•ˆæœã€‚
                
                ---
                
                ## ğŸ”„ æ“ä½œæµç¨‹
                
                1. **ä¸Šä¼ éŸ³é¢‘**: åœ¨å·¦ä¾§ä¸Šä¼ å¾…ç¼–è¾‘çš„éŸ³é¢‘æ–‡ä»¶
                2. **å¡«å†™æ–‡æœ¬**: åœ¨ "Prompt Text" ä¸­å¡«å†™éŸ³é¢‘å¯¹åº”çš„æ–‡æœ¬å†…å®¹
                3. **é€‰æ‹©ä»»åŠ¡**: åœ¨å³ä¾§é€‰æ‹©ä»»åŠ¡ç±»å‹ï¼ˆTaskï¼‰å’Œå­ä»»åŠ¡ï¼ˆSub-taskï¼‰
                4. **ç›®æ ‡æ–‡æœ¬**: å¦‚éœ€ä¿®æ”¹æ–‡æœ¬å†…å®¹ï¼ˆå¦‚å…‹éš†ã€å‰¯è¯­è¨€ï¼‰ï¼Œåœ¨ "Target Text" ä¸­å¡«å†™æ–°æ–‡æœ¬
                5. **ç‚¹å‡»æŒ‰é’®**: ç‚¹å‡» "CLONE" æˆ– "EDIT" æŒ‰é’®ç”ŸæˆéŸ³é¢‘
                
                ---
                
                ## ğŸ·ï¸ å¿«é€Ÿæ ‡ç­¾å‚è€ƒ
                
                ### è¯­è¨€åˆ‡æ¢æ ‡ç­¾ï¼ˆæ”¾åœ¨æ–‡æœ¬æœ€å‰é¢ï¼‰
                ```
                [Sichuanese]  - å››å·è¯
                [Cantonese]   - ç²¤è¯­
                [Japanese]    - æ—¥è¯­
                [Korean]      - éŸ©è¯­
                ï¼ˆæ— æ ‡ç­¾ï¼‰     - æ™®é€šè¯/è‹±æ–‡ï¼ˆè‡ªåŠ¨è¯†åˆ«ï¼‰
                ```
                
                ### å‰¯è¯­è¨€æ ‡ç­¾ï¼ˆå¯æ”¾åœ¨æ–‡æœ¬ä»»æ„ä½ç½®ï¼‰
                ```
                [Breathing]           - å‘¼å¸å£°
                [Laughter]            - ç¬‘å£°
                [Uhm]                 - çŠ¹è±«å£° "å—¯..."
                [Sigh]                - å¹æ°”å£°
                [Surprise-oh]         - æƒŠè®¶ "å“¦ï¼"
                [Surprise-ah]         - æƒŠè®¶ "å•Šï¼"
                [Surprise-wa]         - æƒŠè®¶ "å“‡ï¼"
                [Confirmation-en]     - ç¡®è®¤ "å—¯"
                [Question-ei]         - ç–‘é—® "è¯¶ï¼Ÿ"
                [Dissatisfaction-hnn] - ä¸æ»¡ "å“¼"
                ```
                
                ### å¤šéŸ³å­—æ ‡æ³¨ï¼ˆç”¨æ‹¼éŸ³+å£°è°ƒæ›¿æ¢ï¼‰
                ```
                guo4 = è¿‡ï¼ˆç¬¬4å£°ï¼‰
                zhong4 = é‡ï¼ˆç¬¬4å£°ï¼‰
                ç¤ºä¾‹: æˆ‘ä¹Ÿæƒ³guo4guo4guo1å„¿guo4guo4çš„ç”Ÿæ´»
                ```
                
                ---
                
                ## ğŸ­ ä»»åŠ¡ç±»å‹è¯¦è§£
                
                ### 1ï¸âƒ£ **Clone (å…‹éš†)** - é›¶æ ·æœ¬è¯­éŸ³å…‹éš†
                - **åŠŸèƒ½**: ä½¿ç”¨ 3-10 ç§’å‚è€ƒéŸ³é¢‘å…‹éš†ä»»æ„éŸ³è‰²
                - **æ”¯æŒè¯­è¨€**: ä¸­æ–‡ï¼ˆæ™®é€šè¯ï¼‰ã€è‹±æ–‡ã€å››å·è¯ã€ç²¤è¯­ã€æ—¥è¯­ã€éŸ©è¯­
                - **åŸºç¡€ä½¿ç”¨**:
                  ```
                  1. ä¸Šä¼ å‚è€ƒéŸ³é¢‘ï¼ˆ3-10ç§’æ¸…æ™°éŸ³é¢‘ï¼‰
                  2. Prompt Text: å‚è€ƒéŸ³é¢‘çš„æ–‡æœ¬å†…å®¹
                  3. Target Text: ä½ æƒ³è¦åˆæˆçš„æ–°æ–‡æœ¬
                  4. ç‚¹å‡» "CLONE" æŒ‰é’®
                  ```
                
                - **ğŸŒ è¯­è¨€åˆ‡æ¢æ ‡ç­¾ä½¿ç”¨æ–¹æ³•**:
                  
                  **æ™®é€šè¯ï¼ˆé»˜è®¤ï¼‰**:
                  ```
                  Target Text: ä»Šå¤©å¤©æ°”çœŸä¸é”™ï¼Œæˆ‘ä»¬ä¸€èµ·å»å…¬å›­æ•£æ­¥å§ã€‚
                  ```
                  
                  **å››å·è¯**:
                  ```
                  Target Text: [Sichuanese]ä»Šå¤©å¤©æ°”å·´é€‚å¾—å¾ˆï¼Œæˆ‘ä»¬ä¸€èµ·åˆ‡å…¬å›­è€å“ˆã€‚
                  ```
                  
                  **ç²¤è¯­**:
                  ```
                  Target Text: [Cantonese]ä»Šæ—¥å¤©æ°”å¥½å¥½ï¼Œæˆ‘å“‹ä¸€é½å»å…¬åœ’è¡Œå“å•¦ã€‚
                  ```
                  
                  **æ—¥è¯­**:
                  ```
                  Target Text: [Japanese]ä»Šæ—¥ã¯ã„ã„å¤©æ°—ã§ã™ã­ã€ä¸€ç·’ã«å…¬åœ’ã«æ•£æ­©ã—ã¾ã—ã‚‡ã†ã€‚
                  ```
                  
                  **éŸ©è¯­**:
                  ```
                  Target Text: [Korean]ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë„¤ìš”, í•¨ê»˜ ê³µì›ì— ì‚°ì±…í•˜ëŸ¬ ê°€ìš”.
                  ```
                  
                  **è‹±æ–‡**:
                  ```
                  Target Text: The weather is so nice today, let's go for a walk in the park together.
                  ```
                  
                  âš ï¸ **æ³¨æ„**: è¯­è¨€æ ‡ç­¾å¿…é¡»æ”¾åœ¨æ–‡æœ¬**æœ€å‰é¢**ï¼Œç”¨æ–¹æ‹¬å·åŒ…è£¹
                
                - **ğŸµ å¤šéŸ³å­—æ§åˆ¶**:
                  
                  å°†å¤šéŸ³å­—æ›¿æ¢ä¸ºå¸¦å£°è°ƒçš„æ‹¼éŸ³ï¼ˆ1-4å£°ï¼‰ï¼š
                  ```
                  åŸæ–‡: æˆ‘ä¹Ÿæƒ³è¿‡è¿‡è¿‡å„¿è¿‡è¿‡çš„ç”Ÿæ´»
                  æ ‡æ³¨: æˆ‘ä¹Ÿæƒ³guo4guo4guo1å„¿guo4guo4çš„ç”Ÿæ´»
                  
                  åŸæ–‡: ä»–è¦ç»™æˆ‘ä¸€ä¸ªé‡è¦çš„é‡é‡
                  æ ‡æ³¨: ä»–è¦gei3æˆ‘ä¸€ä¸ªzhong4è¦çš„zhong4é‡
                  ```
                
                ### 2ï¸âƒ£ **Clone_with_emotion (å…‹éš†+æƒ…æ„Ÿ)** - å…‹éš†å¹¶æ·»åŠ æƒ…æ„Ÿ ğŸ†•
                - **åŠŸèƒ½**: ä½¿ç”¨å‚è€ƒéŸ³è‰²è¯´å‡ºæ–°æ–‡æœ¬ï¼Œå¹¶æ·»åŠ æŒ‡å®šæƒ…æ„Ÿ
                - **ä¸¤æ­¥å¤„ç†**:
                  1. å…‹éš†éŸ³è‰²å¹¶ç”Ÿæˆæ–°æ–‡æœ¬
                  2. ä¸ºç”Ÿæˆçš„éŸ³é¢‘æ·»åŠ æƒ…æ„Ÿ
                - **æ”¯æŒæƒ…æ„Ÿ**: happy (å¼€å¿ƒ), angry (ç”Ÿæ°”), sad (æ‚²ä¼¤), fear (ææƒ§), surprised (æƒŠè®¶), excited (å…´å¥‹), depressed (æ²®ä¸§), humour (å¹½é»˜), confusion (å›°æƒ‘), disgusted (åŒæ¶), empathy (åŒæƒ…), embarrass (å°´å°¬), coldness (å†·æ¼ ), admiration (é’¦ä½©)
                - **ä½¿ç”¨åœºæ™¯**: æƒ³è¦ç”¨ç‰¹å®šéŸ³è‰²è¯´æ–°å†…å®¹ï¼Œå¹¶å¸¦æœ‰ç‰¹å®šæƒ…æ„Ÿ
                
                ### 3ï¸âƒ£ **Clone_with_style (å…‹éš†+é£æ ¼)** - å…‹éš†å¹¶æ”¹å˜é£æ ¼ ğŸ†•
                - **åŠŸèƒ½**: ä½¿ç”¨å‚è€ƒéŸ³è‰²è¯´å‡ºæ–°æ–‡æœ¬ï¼Œå¹¶åº”ç”¨æŒ‡å®šè¯´è¯é£æ ¼
                - **ä¸¤æ­¥å¤„ç†**:
                  1. å…‹éš†éŸ³è‰²å¹¶ç”Ÿæˆæ–°æ–‡æœ¬
                  2. ä¸ºç”Ÿæˆçš„éŸ³é¢‘åº”ç”¨é£æ ¼
                - **æ”¯æŒé£æ ¼**: whisper (è€³è¯­), serious (ä¸¥è‚ƒ), child (ç«¥å£°), older (è€å¹´), sweet (ç”œç¾), gentle (æ¸©æŸ”), warm (æ¸©æš–), authority (æƒå¨), chat (èŠå¤©), radio (æ’­éŸ³), story (è®²æ•…äº‹), news (æ–°é—»), advertising (å¹¿å‘Š) ç­‰ 32 ç§é£æ ¼
                - **ä½¿ç”¨åœºæ™¯**: æƒ³è¦ç”¨ç‰¹å®šéŸ³è‰²è¯´æ–°å†…å®¹ï¼Œå¹¶å¸¦æœ‰ç‰¹å®šè¯´è¯é£æ ¼
                
                ### 4ï¸âƒ£ **Emotion (æƒ…æ„Ÿ)** - æƒ…æ„Ÿç¼–è¾‘
                - **åŠŸèƒ½**: ä¸ºç°æœ‰éŸ³é¢‘æ·»åŠ æˆ–å¢å¼ºæƒ…æ„Ÿè¡¨è¾¾
                - **è¿­ä»£æ§åˆ¶**: æ”¯æŒå¤šæ¬¡è¿­ä»£ï¼Œé€æ­¥å¢å¼ºæƒ…æ„Ÿå¼ºåº¦
                - **æ”¯æŒæƒ…æ„Ÿ**: 14 ç§æƒ…æ„Ÿ + remove (ç§»é™¤æƒ…æ„Ÿ)
                - **ä½¿ç”¨æ–¹æ³•**:
                  - ä¸Šä¼ éŸ³é¢‘å¹¶å¡«å†™å¯¹åº”æ–‡æœ¬
                  - é€‰æ‹©ç›®æ ‡æƒ…æ„Ÿ
                  - è°ƒæ•´å¼ºåº¦ï¼ˆIntensity: 1-3ï¼‰
                  - ç‚¹å‡» "EDIT" æŒ‰é’®
                - **æç¤º**: å¯å¤šæ¬¡ç‚¹å‡» "EDIT" å åŠ æ•ˆæœ
                
                ### 5ï¸âƒ£ **Style (é£æ ¼)** - è¯´è¯é£æ ¼ç¼–è¾‘
                - **åŠŸèƒ½**: æ”¹å˜éŸ³é¢‘çš„è¯´è¯é£æ ¼å’Œè¡¨è¾¾æ–¹å¼
                - **æ”¯æŒé£æ ¼**: 32 ç§é£æ ¼ + remove (ç§»é™¤é£æ ¼)
                - **ç‰¹æ®Šé£æ ¼è¯´æ˜**:
                  - **whisper (è€³è¯­)**: å»ºè®®è¿­ä»£æ¬¡æ•° â‰¥ 2 ä»¥è·å¾—æ›´å¥½æ•ˆæœ
                  - **child (ç«¥å£°)** / **older (è€å¹´)**: æ”¹å˜éŸ³è‰²å¹´é¾„æ„Ÿ
                  - **act_coy (æ’’å¨‡)**: ç”œç¾ã€ä¿çš®ã€äº²æ˜µçš„è¡¨è¾¾æ–¹å¼
                  - **radio (æ’­éŸ³)** / **news (æ–°é—»)**: ä¸“ä¸šæ’­éŸ³é£æ ¼
                
                ### 6ï¸âƒ£ **Paralinguistic (å‰¯è¯­è¨€)** - å‰¯è¯­è¨€ç‰¹å¾ç¼–è¾‘
                - **åŠŸèƒ½**: æ·»åŠ éè¯­è¨€å£°éŸ³ï¼Œä½¿éŸ³é¢‘æ›´è‡ªç„¶ã€æ›´å…·è¡¨ç°åŠ›
                
                - **ğŸ“¢ æ”¯æŒçš„å‰¯è¯­è¨€æ ‡ç­¾**:
                  
                  | æ ‡ç­¾ | ä¸­æ–‡è¯´æ˜ | ä½¿ç”¨åœºæ™¯ |
                  |------|---------|---------|
                  | `[Breathing]` | å‘¼å¸å£° | è¡¨è¾¾ç´§å¼ ã€ç–²æƒ«ã€æ”¾æ¾ |
                  | `[Laughter]` | ç¬‘å£° | è¡¨è¾¾å¼€å¿ƒã€å¹½é»˜ |
                  | `[Uhm]` | çŠ¹è±«å£° "å—¯..." | æ€è€ƒã€çŠ¹è±«ã€ä¸ç¡®å®š |
                  | `[Sigh]` | å¹æ°”å£° | æ— å¥ˆã€å¤±æœ›ã€æ”¾æ¾ |
                  | `[Surprise-oh]` | æƒŠè®¶å£° "å“¦ï¼" | è½»å¾®æƒŠè®¶ã€æç„¶å¤§æ‚Ÿ |
                  | `[Surprise-ah]` | æƒŠè®¶å£° "å•Šï¼" | å¼ºçƒˆæƒŠè®¶ã€éœ‡æƒŠ |
                  | `[Surprise-wa]` | æƒŠè®¶å£° "å“‡ï¼" | èµå¹ã€æƒŠå–œ |
                  | `[Confirmation-en]` | ç¡®è®¤å£° "å—¯" | åŒæ„ã€ç¡®è®¤ã€ç†è§£ |
                  | `[Question-ei]` | ç–‘é—®å£° "è¯¶ï¼Ÿ" | ç–‘æƒ‘ã€è¯¢é—® |
                  | `[Dissatisfaction-hnn]` | ä¸æ»¡å£° "å“¼" | ä¸æ»¡ã€è½»è”‘ã€å‚²å¨‡ |
                
                - **ğŸ¯ è¯¦ç»†ä½¿ç”¨ç¤ºä¾‹**:
                  
                  **ç¤ºä¾‹ 1: æ·»åŠ ç¬‘å£°**
                  ```
                  æ­¥éª¤1 - åŸºç¡€å…‹éš†:
                  Prompt Text: ä»Šå¤©å¤©æ°”çœŸä¸é”™
                  Target Text: ä»Šå¤©å¤©æ°”çœŸä¸é”™
                  ç‚¹å‡» "CLONE"
                  
                  æ­¥éª¤2 - æ·»åŠ ç¬‘å£°:
                  Target Text: ä»Šå¤©å¤©æ°”çœŸä¸é”™[Laughter]
                  Task: paralinguistic (å‰¯è¯­è¨€)
                  ç‚¹å‡» "EDIT"
                  ```
                  
                  **ç¤ºä¾‹ 2: å¤šä¸ªæ ‡ç­¾ç»„åˆ**
                  ```
                  Target Text: [Uhm]æˆ‘è§‰å¾—è¿™ä¸ªæ–¹æ¡ˆ[Breathing]å¯èƒ½éœ€è¦å†è€ƒè™‘ä¸€ä¸‹[Sigh]
                  æ•ˆæœ: çŠ¹è±« + å‘¼å¸ + å¹æ°”ï¼Œè¡¨è¾¾çº ç»“çš„å¿ƒæƒ…
                  ```
                  
                  **ç¤ºä¾‹ 3: è¡¨è¾¾æƒŠå–œ**
                  ```
                  Target Text: [Surprise-wa]è¿™ä¸ªç¤¼ç‰©å¤ªæ£’äº†[Laughter]ï¼Œè°¢è°¢ä½ ï¼
                  æ•ˆæœ: æƒŠå–œçš„èµå¹ + å¼€å¿ƒçš„ç¬‘å£°
                  ```
                  
                  **ç¤ºä¾‹ 4: è¡¨è¾¾ä¸æ»¡**
                  ```
                  Target Text: ä½ åˆå¿˜è®°å¸¦é’¥åŒ™äº†[Dissatisfaction-hnn]ï¼ŒçœŸæ˜¯æ‹¿ä½ æ²¡åŠæ³•ã€‚
                  æ•ˆæœ: æ— å¥ˆçš„ä¸æ»¡å£°
                  ```
                  
                  **ç¤ºä¾‹ 5: æ€è€ƒçŠ¹è±«**
                  ```
                  Target Text: [Uhm]è¿™ä¸ªé—®é¢˜[Breathing]è®©æˆ‘æƒ³æƒ³[Uhm]ï¼Œå¤§æ¦‚æ˜¯è¿™æ ·çš„ã€‚
                  æ•ˆæœ: æ€è€ƒä¸­çš„çŠ¹è±«å’Œåœé¡¿
                  ```
                  
                  **ç¤ºä¾‹ 6: ç¡®è®¤ç†è§£**
                  ```
                  Target Text: [Confirmation-en]æˆ‘æ˜ç™½äº†ï¼Œ[Confirmation-en]å°±æŒ‰ä½ è¯´çš„åŠã€‚
                  æ•ˆæœ: è¡¨è¾¾ç†è§£å’ŒåŒæ„
                  ```
                  
                  **ç¤ºä¾‹ 7: ç–‘é—®è¯¢é—®**
                  ```
                  Target Text: [Question-ei]ä½ è¯´ä»€ä¹ˆï¼Ÿæˆ‘æ²¡å¬æ¸…æ¥š[Question-ei]
                  æ•ˆæœ: ç–‘æƒ‘çš„è¯¢é—®
                  ```
                  
                  **ç¤ºä¾‹ 8: æç„¶å¤§æ‚Ÿ**
                  ```
                  Target Text: [Surprise-oh]åŸæ¥æ˜¯è¿™æ ·å•Šï¼Œæˆ‘æ‡‚äº†ï¼
                  æ•ˆæœ: çªç„¶æ˜ç™½çš„æ„Ÿè§‰
                  ```
                  
                  âš ï¸ **é‡è¦æç¤º**:
                  - æ ‡ç­¾å¯ä»¥æ”¾åœ¨å¥å­ä¸­çš„**ä»»æ„ä½ç½®**
                  - å¯ä»¥åœ¨ä¸€å¥è¯ä¸­ä½¿ç”¨**å¤šä¸ªæ ‡ç­¾**
                  - æ ‡ç­¾ä¼šåœ¨è¯¥ä½ç½®æ’å…¥å¯¹åº”çš„å£°éŸ³
                  - å»ºè®®å…ˆç”¨ CLONE ç”ŸæˆåŸºç¡€éŸ³é¢‘ï¼Œå†ç”¨ EDIT æ·»åŠ å‰¯è¯­è¨€ç‰¹å¾
                
                ### 7ï¸âƒ£ **VAD (è¯­éŸ³æ´»åŠ¨æ£€æµ‹)** - é™éŸ³ç§»é™¤
                - **åŠŸèƒ½**: è‡ªåŠ¨ç§»é™¤éŸ³é¢‘ä¸­çš„é™éŸ³éƒ¨åˆ†ï¼Œä¿ç•™è¯­éŸ³å†…å®¹
                - **ä½¿ç”¨æ–¹æ³•**: ä¸Šä¼ éŸ³é¢‘ï¼Œé€‰æ‹© vad ä»»åŠ¡ï¼Œç‚¹å‡» "EDIT"
                - **æ³¨æ„**: æ— éœ€å¡«å†™æ–‡æœ¬
                
                ### 8ï¸âƒ£ **Denoise (é™å™ª)** - å™ªéŸ³ç§»é™¤
                - **åŠŸèƒ½**: ç§»é™¤éŸ³é¢‘ä¸­çš„èƒŒæ™¯å™ªéŸ³ï¼Œä¿æŒè¯­éŸ³æ¸…æ™°
                - **ä½¿ç”¨æ–¹æ³•**: ä¸Šä¼ éŸ³é¢‘ï¼Œé€‰æ‹© denoise ä»»åŠ¡ï¼Œç‚¹å‡» "EDIT"
                - **æ³¨æ„**: æ— éœ€å¡«å†™æ–‡æœ¬
                - **æ•ˆæœ**: åœ¨ä¿æŒè¯­éŸ³è´¨é‡çš„åŒæ—¶æ¶ˆé™¤å™ªéŸ³
                
                ### 9ï¸âƒ£ **Speed (è¯­é€Ÿ)** - è¯­é€Ÿè°ƒæ•´
                - **åŠŸèƒ½**: è°ƒæ•´éŸ³é¢‘çš„è¯´è¯é€Ÿåº¦
                - **æ”¯æŒé€‰é¡¹**:
                  - `faster (æ›´å¿«)` - è½»å¾®åŠ å¿«
                  - `slower (æ›´æ…¢)` - è½»å¾®å‡æ…¢
                  - `more faster (éå¸¸å¿«)` - æ˜¾è‘—åŠ å¿«
                  - `more slower (éå¸¸æ…¢)` - æ˜¾è‘—å‡æ…¢
                - **ä½¿ç”¨æ–¹æ³•**: ä¸Šä¼ éŸ³é¢‘ï¼Œå¡«å†™æ–‡æœ¬ï¼Œé€‰æ‹©é€Ÿåº¦é€‰é¡¹ï¼Œç‚¹å‡» "EDIT"
                
                ---
                
                ## ğŸ’¡ é«˜çº§æŠ€å·§
                
                ### ğŸ”„ è¿­ä»£ç¼–è¾‘
                - å¯ä»¥å¤šæ¬¡ç‚¹å‡» "EDIT" æŒ‰é’®ï¼Œé€æ­¥å¢å¼ºæ•ˆæœ
                - æ¯æ¬¡ç¼–è¾‘éƒ½ä¼šåœ¨ä¸Šä¸€æ¬¡ç»“æœçš„åŸºç¡€ä¸Šå åŠ 
                - å†å²è®°å½•ä¼šä¿å­˜æ‰€æœ‰ç¼–è¾‘æ­¥éª¤
                
                ### ğŸšï¸ å¼ºåº¦æ§åˆ¶
                - **Intensity (å¼ºåº¦)**: 1.0 - 3.0
                - 1.0: è½»å¾®æ•ˆæœ
                - 2.0: ä¸­ç­‰æ•ˆæœï¼ˆæ¨èï¼‰
                - 3.0: å¼ºçƒˆæ•ˆæœ
                
                ### ğŸ­ ç»„åˆä½¿ç”¨
                - å…ˆä½¿ç”¨ **clone_with_emotion** æˆ– **clone_with_style** ç”Ÿæˆå¸¦æƒ…æ„Ÿ/é£æ ¼çš„æ–°æ–‡æœ¬éŸ³é¢‘
                - å†ä½¿ç”¨ **paralinguistic** æ·»åŠ å‰¯è¯­è¨€ç‰¹å¾
                - æœ€åä½¿ç”¨ **speed** è°ƒæ•´è¯­é€Ÿ
                
                ### ğŸ“ æœ€ä½³å®è·µ
                - **éŸ³é¢‘é•¿åº¦**: å»ºè®®æ¯æ¬¡æ¨ç†éŸ³é¢‘ä¸è¶…è¿‡ 30 ç§’
                - **å‚è€ƒéŸ³é¢‘**: 3-10 ç§’æ¸…æ™°éŸ³é¢‘æ•ˆæœæœ€ä½³
                - **æ–‡æœ¬åŒ¹é…**: ç¡®ä¿æ–‡æœ¬ä¸éŸ³é¢‘å†…å®¹å®Œå…¨åŒ¹é…
                - **è¿­ä»£æ¬¡æ•°**: whisper é£æ ¼å»ºè®® 2+ æ¬¡è¿­ä»£
                
                ---
                
                ## ğŸš€ æ€§èƒ½ä¼˜åŒ–
                
                ### GPU å†…å­˜ç®¡ç†
                - **å¯åŠ¨å†…å­˜**: 3 MBï¼ˆæ‡’åŠ è½½ï¼‰
                - **æ¨ç†å†…å­˜**: 40 GBï¼ˆå³°å€¼ï¼‰
                - **ç©ºé—²å†…å­˜**: 5.7 GBï¼ˆè‡ªåŠ¨å¸è½½ï¼‰
                - **èŠ‚çœ**: ç›¸æ¯”ä¼ ç»Ÿæ–¹å¼èŠ‚çœ 85% å†…å­˜
                
                ### é€Ÿåº¦ä¼˜åŒ–
                - **FunASR ç¼“å­˜**: é¦–æ¬¡æ¨ç†åè‡ªåŠ¨ç¼“å­˜ï¼Œåç»­æ¨ç†åŠ é€Ÿ 3 å€
                - **é¦–æ¬¡åŠ è½½**: 20-30 ç§’ï¼ˆä¸€æ¬¡æ€§æˆæœ¬ï¼‰
                - **åç»­æ¨ç†**: 8-24 ç§’ï¼ˆå«ç¼“å­˜ï¼‰
                
                ---
                
                ## âš ï¸ æ³¨æ„äº‹é¡¹
                
                1. **åˆæ³•ä½¿ç”¨**: è¯·å‹¿ç”¨äºæœªç»æˆæƒçš„è¯­éŸ³å…‹éš†ã€èº«ä»½å†’å……ã€æ¬ºè¯ˆã€æ·±åº¦ä¼ªé€ æˆ–å…¶ä»–éæ³•ç›®çš„
                2. **ä¼¦ç†è§„èŒƒ**: ç¡®ä¿éµå®ˆå½“åœ°æ³•å¾‹æ³•è§„å’Œä¼¦ç†å‡†åˆ™
                3. **è´£ä»»å£°æ˜**: å¼€å‘è€…ä¸å¯¹æŠ€æœ¯æ»¥ç”¨è´Ÿè´£
                4. **éŸ³é¢‘è´¨é‡**: å‚è€ƒéŸ³é¢‘è´¨é‡ç›´æ¥å½±å“å…‹éš†æ•ˆæœ
                5. **æ–‡æœ¬å‡†ç¡®**: æ–‡æœ¬ä¸éŸ³é¢‘å†…å®¹å¿…é¡»åŒ¹é…ï¼Œå¦åˆ™å½±å“ç¼–è¾‘æ•ˆæœ
                
                ---
                
                ## ğŸ”— ç›¸å…³é“¾æ¥
                
                - ğŸ“„ [æŠ€æœ¯æŠ¥å‘Š](https://arxiv.org/abs/2511.03601)
                - ğŸ® [åœ¨çº¿æ¼”ç¤º](https://stepaudiollm.github.io/step-audio-editx/)
                - ğŸ¤— [HuggingFace æ¨¡å‹](https://huggingface.co/stepfun-ai/Step-Audio-EditX)
                - ğŸŒ [ModelScope æ¨¡å‹](https://modelscope.cn/models/stepfun-ai/Step-Audio-EditX)
                - ğŸ“Š [è¯„æµ‹åŸºå‡†](https://github.com/stepfun-ai/Step-Audio-Edit-Benchmark)
                """)
            
            # é¡¹ç›®ä¿¡æ¯åŒºåŸŸ
            gr.Markdown("---")
            gr.Markdown("""
            ## ğŸ‘¥ å…³äºæœ¬é¡¹ç›®
            
            ### åŸå§‹é¡¹ç›®
            - **é¡¹ç›®åç§°**: Step-Audio-EditX
            - **å¼€å‘å›¢é˜Ÿ**: Stepfun AI (é˜¶è·ƒæ˜Ÿè¾°)
            - **æ¨¡å‹è§„æ¨¡**: 3B å‚æ•°
            - **æŠ€æœ¯æ¶æ„**: LLM-based Reinforcement Learning Audio Model
            - **å¼€æºåè®®**: Apache 2.0 License
            
            ### GPU å†…å­˜ç®¡ç†ç‰ˆæœ¬
            - **ä¼˜åŒ–ä½œè€…**: [@neosun100](https://github.com/neosun100)
            - **é¡¹ç›®ä»“åº“**: [Step-Audio-EditX-GPU-Managed](https://github.com/neosun100/Step-Audio-EditX-GPU-Managed)
            - **ä¸»è¦æ”¹è¿›**:
              - âœ… å®ç°æ‡’åŠ è½½ï¼Œå¯åŠ¨å†…å­˜ä» 40GB é™è‡³ 3MBï¼ˆ99.99% èŠ‚çœï¼‰
              - âœ… è‡ªåŠ¨ GPUâ†”CPU å¸è½½ï¼Œç©ºé—²å†…å­˜é™è‡³ 5.7GBï¼ˆ85% èŠ‚çœï¼‰
              - âœ… æ–°å¢ clone_with_emotion å’Œ clone_with_style åŠŸèƒ½
              - âœ… åŒè¯­ UIï¼ˆä¸­è‹±æ–‡ï¼‰
              - âœ… å®æ—¶æ—¥å¿—å’Œ GPU çŠ¶æ€ç›‘æ§
              - âœ… FunASR æŒä¹…åŒ–ç¼“å­˜ï¼ˆ3å€åŠ é€Ÿï¼‰
            
            ### ğŸŒŸ æ”¯æŒæœ¬é¡¹ç›®
            
            å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œæ¬¢è¿ï¼š
            - â­ åœ¨ [GitHub](https://github.com/neosun100/Step-Audio-EditX-GPU-Managed) ä¸Šç»™é¡¹ç›®ç‚¹ Star
            - ğŸ› æäº¤ [Issue](https://github.com/neosun100/Step-Audio-EditX-GPU-Managed/issues) æŠ¥å‘Šé—®é¢˜
            - ğŸ’¡ åœ¨ [Discussions](https://github.com/neosun100/Step-Audio-EditX-GPU-Managed/discussions) åˆ†äº«æƒ³æ³•
            - ğŸ”€ æäº¤ [Pull Request](https://github.com/neosun100/Step-Audio-EditX-GPU-Managed/pulls) è´¡çŒ®ä»£ç 
            - ğŸ“¢ åˆ†äº«ç»™æ›´å¤šéœ€è¦çš„äºº
            
            ### ğŸ“ è”ç³»æ–¹å¼
            
            - **åŸå§‹é¡¹ç›®**: [stepfun-ai/Step-Audio-EditX](https://github.com/stepfun-ai/Step-Audio-EditX)
            - **GPU ç®¡ç†ç‰ˆ**: [neosun100/Step-Audio-EditX-GPU-Managed](https://github.com/neosun100/Step-Audio-EditX-GPU-Managed)
            - **é—®é¢˜åé¦ˆ**: [æäº¤ Issue](https://github.com/neosun100/Step-Audio-EditX-GPU-Managed/issues/new)
            
            ### ğŸ™ è‡´è°¢
            
            æ„Ÿè°¢ä»¥ä¸‹å¼€æºé¡¹ç›®çš„è´¡çŒ®ï¼š
            - [CosyVoice](https://github.com/FunAudioLLM/CosyVoice) - TTS æ¨¡å‹
            - [FunASR](https://github.com/alibaba-damo-academy/FunASR) - éŸ³é¢‘åˆ†è¯
            - [Whisper](https://github.com/openai/whisper) - è¯­éŸ³è½¬æ–‡å­—
            - [Transformers](https://github.com/huggingface/transformers) - æ¨¡å‹æ¡†æ¶
            
            ---
            
            **Made with â¤ï¸ by the community | ç‰ˆæœ¬: GPU-Managed v1.0 | æ›´æ–°æ—¶é—´: 2025-12-05**
            """)

    def register_events(self):
        """Register event handlers"""
        # Create independent state for each session
        state = gr.State(self.init_state())

        self.button_tts.click(self.generate_clone,
            inputs=[self.prompt_text_input, self.prompt_audio_input, self.generated_text, self.edit_type, self.edit_info, self.model_variant, self.intensity, state],
            outputs=[self.chat_box, state, self.cache_stats_display, self.live_log_display])
        self.button_edit.click(self.generate_edit,
            inputs=[self.prompt_text_input, self.prompt_audio_input, self.generated_text, self.edit_type, self.edit_info, self.model_variant, self.intensity, state],
            outputs=[self.chat_box, state])
        
        # Cache control events
        self.refresh_cache_btn.click(
            fn=self.get_cache_stats,
            inputs=[],
            outputs=self.cache_stats_display
        )
        self.clear_cache_btn.click(
            fn=self.clear_cache,
            inputs=[],
            outputs=self.cache_stats_display
        )
        
        # Log control events
        self.refresh_log_btn.click(
            fn=self.get_live_logs,
            inputs=[],
            outputs=self.live_log_display
        )
        self.clear_log_btn.click(
            fn=self.clear_live_logs,
            inputs=[],
            outputs=self.live_log_display
        )
        
        # GPU management events (if enabled)
        if self.args.enable_gpu_management:
            self.refresh_gpu_btn.click(
                fn=self.get_gpu_status,
                inputs=[],
                outputs=self.gpu_status_display
            )
            self.offload_gpu_btn.click(
                fn=self.offload_gpu,
                inputs=[],
                outputs=[gr.Textbox(visible=False), self.gpu_status_display]
            )
            self.release_gpu_btn.click(
                fn=self.release_gpu,
                inputs=[],
                outputs=[gr.Textbox(visible=False), self.gpu_status_display]
            )

        self.clean_history_submit.click(self.clear_history, inputs=[state], outputs=[self.chat_box, state])
        self.edit_type.change(
            fn=self.update_edit_info,
            inputs=self.edit_type,
            outputs=self.edit_info,
        )

        # Add audio transcription event only if enabled
        if self.enable_auto_transcribe:
            self.prompt_audio_input.change(
                fn=self.transcribe_audio,
                inputs=[self.prompt_audio_input, self.prompt_text_input],
                outputs=self.prompt_text_input,
            )

    def update_edit_info(self, category):
        """Update sub-task dropdown based on main task selection"""
        category_items = get_supported_edit_types()
        choices = category_items.get(category, [])
        value = None if len(choices) == 0 else choices[0]
        return gr.Dropdown(label="Sub-task", choices=choices, value=value)
    
    def get_cache_stats(self):
        """è·å– FunASR ç¼“å­˜ç»Ÿè®¡ï¼ˆè¿”å›æ ¼å¼åŒ–æ–‡æœ¬ï¼‰"""
        return self.format_cache_stats()
    
    def format_cache_stats(self):
        """æ ¼å¼åŒ–ç¼“å­˜ç»Ÿè®¡ä¸ºæ˜“è¯»æ–‡æœ¬"""
        if not hasattr(self, 'encoder'):
            return "âŒ é”™è¯¯ï¼šEncoder æœªåˆå§‹åŒ–"
        
        if not hasattr(self.encoder, 'get_cache_stats'):
            return "âŒ é”™è¯¯ï¼šEncoder æ²¡æœ‰ get_cache_stats æ–¹æ³•"
        
        try:
            stats = self.encoder.get_cache_stats()
            self.logger.info(f"âœ… Retrieved cache stats: {stats}")
            
            # æ ¼å¼åŒ–ä¸ºæ˜“è¯»æ–‡æœ¬
            text = "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
            text += "ğŸ“Š FunASR ç¼“å­˜æ€§èƒ½ç»Ÿè®¡\n"
            text += "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"
            
            if "error" in stats:
                text += f"âŒ {stats.get('error')}\n"
                text += f"   {stats.get('info', '')}\n"
            else:
                text += f"âœ… ç¼“å­˜çŠ¶æ€ï¼š{'å¯ç”¨' if stats.get('enabled') else 'ç¦ç”¨'}\n\n"
                text += f"ğŸ“ˆ ç»Ÿè®¡æ•°æ®ï¼š\n"
                text += f"   â€¢ å‘½ä¸­æ¬¡æ•°ï¼š{stats.get('hits', 0)} æ¬¡\n"
                text += f"   â€¢ æœªå‘½ä¸­æ¬¡æ•°ï¼š{stats.get('misses', 0)} æ¬¡\n"
                text += f"   â€¢ æ€»è¯·æ±‚æ•°ï¼š{stats.get('total_requests', 0)} æ¬¡\n"
                text += f"   â€¢ å‘½ä¸­ç‡ï¼š{stats.get('hit_rate', '0.0%')}\n\n"
                text += f"ğŸ’¾ ç¼“å­˜ä½¿ç”¨ï¼š\n"
                text += f"   â€¢ å½“å‰å¤§å°ï¼š{stats.get('cache_size', 0)} é¡¹\n"
                text += f"   â€¢ æœ€å¤§å®¹é‡ï¼š{stats.get('max_size', 0)} é¡¹\n\n"
                text += f"â±ï¸ æ€§èƒ½æå‡ï¼š\n"
                text += f"   â€¢ é¢„ä¼°èŠ‚çœæ—¶é—´ï¼š{stats.get('time_saved_estimate', '0s')}\n"
                text += f"   â€¢ æ¯æ¬¡å‘½ä¸­èŠ‚çœï¼š~1.65s\n\n"
                
                # æ·»åŠ æ€§èƒ½å»ºè®®
                hit_rate_num = float(stats.get('hit_rate', '0%').rstrip('%'))
                if hit_rate_num > 50:
                    text += "ğŸ‰ ç¼“å­˜æ•ˆæœå¾ˆå¥½ï¼\n"
                elif hit_rate_num > 0:
                    text += "ğŸ’¡ æç¤ºï¼šä½¿ç”¨ç›¸åŒéŸ³é¢‘å¯æé«˜å‘½ä¸­ç‡\n"
                else:
                    text += "ğŸ’¡ æç¤ºï¼šæ‰§è¡Œå‡ æ¬¡ clone åæŸ¥çœ‹æ•ˆæœ\n"
            
            text += "\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
            text += f"ğŸ• æ›´æ–°æ—¶é—´ï¼š{time.strftime('%H:%M:%S')}\n"
            
            return text
            
        except Exception as e:
            self.logger.error(f"Failed to get cache stats: {e}")
            return f"âŒ è·å–ç»Ÿè®¡å¤±è´¥ï¼š{str(e)}"
    
    def clear_cache(self):
        """æ¸…ç©º FunASR ç¼“å­˜"""
        if hasattr(self, 'encoder') and hasattr(self.encoder, 'clear_cache'):
            self.encoder.clear_cache()
            self.logger.info("ğŸ—‘ï¸ Cache cleared")
            return self.format_cache_stats()
        return "âŒ é”™è¯¯ï¼šCache not available"
    
    def add_log(self, message):
        """æ·»åŠ æ—¥å¿—æ¡ç›®ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰"""
        timestamp = time.strftime("%H:%M:%S")
        log_entry = f"[{timestamp}] {message}"
        self.live_logs.append(log_entry)
        # Keep only the last max_logs entries
        if len(self.live_logs) > self.max_logs:
            self.live_logs = self.live_logs[-self.max_logs:]
    
    def get_live_logs(self):
        """è·å–æ ¼å¼åŒ–çš„å®æ—¶æ—¥å¿—"""
        if not self.live_logs:
            return "æš‚æ— æ—¥å¿—è®°å½•\næ‰§è¡Œ CLONE/EDIT æ“ä½œåå°†æ˜¾ç¤ºæ—¥å¿—"
        
        # Return last 50 logs (most recent)
        recent_logs = self.live_logs[-50:]
        return "\n".join(recent_logs)
    
    def clear_live_logs(self):
        """æ¸…ç©ºå®æ—¶æ—¥å¿—"""
        self.live_logs.clear()
        self.add_log("ğŸ“‹ æ—¥å¿—å·²æ¸…ç©º")
        return self.get_live_logs()
    
    def get_gpu_status(self):
        """è·å– GPU çŠ¶æ€"""
        if not self.args.enable_gpu_management or not self.tts_engine:
            return "GPU ç®¡ç†æœªå¯ç”¨"
        
        try:
            status = self.tts_engine.get_gpu_status()
            
            if not status.get('enabled', True):
                return "GPU ç®¡ç†æœªå¯ç”¨"
            
            # æ ¼å¼åŒ–æ˜¾ç¤º
            lines = []
            lines.append(f"ğŸ® GPU æ˜¾å­˜ç®¡ç†çŠ¶æ€")
            lines.append(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
            lines.append(f"GPU æ˜¾å­˜å ç”¨: {status.get('gpu_memory_mb', 0):.1f} MB")
            lines.append(f"ç©ºé—²è¶…æ—¶: {status.get('idle_timeout', 0)} ç§’")
            lines.append("")
            
            models = status.get('models', {})
            if models:
                lines.append("ğŸ“¦ æ¨¡å‹çŠ¶æ€:")
                for model_name, model_info in models.items():
                    location = model_info.get('location', 'unknown')
                    idle_sec = model_info.get('idle_seconds', 0)
                    
                    location_icon = {
                        'gpu': 'ğŸŸ¢ GPU',
                        'cpu': 'ğŸŸ¡ CPU',
                        'unloaded': 'âšª æœªåŠ è½½'
                    }.get(location, 'â“ æœªçŸ¥')
                    
                    lines.append(f"  â€¢ {model_name}: {location_icon}")
                    lines.append(f"    ç©ºé—²æ—¶é—´: {idle_sec} ç§’")
            else:
                lines.append("ğŸ“¦ æš‚æ— æ¨¡å‹åŠ è½½")
            
            return "\n".join(lines)
        except Exception as e:
            return f"âŒ è·å–çŠ¶æ€å¤±è´¥: {str(e)}"
    
    def offload_gpu(self):
        """æ‰‹åŠ¨å¸è½½ GPU åˆ° CPU"""
        if not self.args.enable_gpu_management or not self.tts_engine:
            return "GPU ç®¡ç†æœªå¯ç”¨", self.get_gpu_status()
        
        try:
            self.tts_engine.force_offload()
            return "âœ… æ¨¡å‹å·²å¸è½½åˆ° CPU", self.get_gpu_status()
        except Exception as e:
            return f"âŒ å¸è½½å¤±è´¥: {str(e)}", self.get_gpu_status()
    
    def release_gpu(self):
        """å®Œå…¨é‡Šæ”¾ GPU æ˜¾å­˜"""
        if not self.args.enable_gpu_management or not self.tts_engine:
            return "GPU ç®¡ç†æœªå¯ç”¨", self.get_gpu_status()
        
        try:
            self.tts_engine.force_release()
            return "âœ… æ¨¡å‹å·²å®Œå…¨é‡Šæ”¾", self.get_gpu_status()
        except Exception as e:
            return f"âŒ é‡Šæ”¾å¤±è´¥: {str(e)}", self.get_gpu_status()

    def transcribe_audio(self, audio_input, current_text):
        """Transcribe audio using Whisper ASR when prompt text is empty"""
        # Only transcribe if current text is empty
        if current_text and current_text.strip():
            return current_text  # Keep existing text
        if not audio_input:
            return ""  # No audio to transcribe
        if whisper_asr is None:
            self.logger.error("Whisper ASR not initialized.")
            return ""

        try:
            # Transcribe audio
            transcribed_text = whisper_asr(audio_input)
            self.logger.info(f"Audio transcribed: {transcribed_text}")
            return transcribed_text

        except Exception as e:
            self.logger.error(f"Failed to transcribe audio: {e}")
            return ""


def launch_demo(args, editx_tab, encoder, tts_engines, whisper_asr_instance):
    """Launch the gradio demo with optional API support"""
    with gr.Blocks(
            theme=gr.themes.Soft(), 
            title="ğŸ™ï¸ Step-Audio-EditX",
            css="""
    :root {
        --font: "Helvetica Neue", Helvetica, Arial, sans-serif;
        --font-mono: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
    }
    """) as demo:
        gr.Markdown("## ğŸ™ï¸ Step-Audio-EditX")
        gr.Markdown("Audio Editing and Zero-Shot Cloning using Step-Audio-EditX")

        # Register components
        editx_tab.register_components()

        # Register events
        editx_tab.register_events()

    # Check if API should be enabled
    enable_api = getattr(args, 'enable_api', False)
    
    if enable_api:
        # Import API components
        from pathlib import Path
        from api_server import build_fastapi_app
        
        logger.info("ğŸ”Œ å¯ç”¨ API æ”¯æŒï¼Œå…±äº«æ¨¡å‹å®ä¾‹")
        
        # Build FastAPI app with shared models
        model_path = Path(args.model_path)
        asset_roots = [model_path.parent / "examples"] if (model_path.parent / "examples").exists() else []
        
        api_app = build_fastapi_app(
            model_engines=tts_engines,
            model_root=model_path,
            asset_roots=asset_roots,
            whisper_asr=whisper_asr_instance
        )
        
        # Mount Gradio to FastAPI
        app = gr.mount_gradio_app(api_app, demo, path="/")
        
        logger.info("=" * 80)
        logger.info(f"âœ“ ç»Ÿä¸€æœåŠ¡å™¨å¯åŠ¨æˆåŠŸ")
        logger.info(f"UI ç•Œé¢: http://{args.server_name}:{args.server_port}")
        logger.info(f"API æ–‡æ¡£: http://{args.server_name}:{args.server_port}/docs")
        logger.info(f"å¥åº·æ£€æŸ¥: http://{args.server_name}:{args.server_port}/healthz")
        logger.info(f"å…±äº«æ¨¡å‹: UI å’Œ API ä½¿ç”¨åŒä¸€ä¸ªæ¨¡å‹å®ä¾‹")
        logger.info("=" * 80)
        
        # Use uvicorn to run the combined app
        import uvicorn
        uvicorn.run(
            app,
            host=args.server_name,
            port=args.server_port,
            log_level="info"
        )
    else:
        # Launch demo only (original behavior)
        demo.queue().launch(
            server_name=args.server_name,
            server_port=args.server_port,
            share=args.share if hasattr(args, 'share') else False
        )


if __name__ == "__main__":
    # Parse command line arguments
    parser = argparse.ArgumentParser(description="Step-Audio Edit Demo")
    parser.add_argument("--model-path", type=str, required=True, help="Model path.")
    parser.add_argument("--server-name", type=str, default="0.0.0.0", help="Demo server name.")
    parser.add_argument("--server-port", type=int, default=7860, help="Demo server port.")
    parser.add_argument("--tmp-dir", type=str, default="/tmp/gradio", help="Save path.")
    parser.add_argument("--share", action="store_true", help="Share gradio app.")

    # Multi-source loading support parameters
    parser.add_argument(
        "--model-source",
        type=str,
        default="auto",
        choices=["auto", "local", "modelscope", "huggingface"],
        help="Model source: auto (detect automatically), local, modelscope, or huggingface"
    )
    parser.add_argument(
        "--tokenizer-model-id",
        type=str,
        default="dengcunqin/speech_paraformer-large_asr_nat-zh-cantonese-en-16k-vocab8501-online",
        help="Tokenizer model ID for online loading"
    )
    parser.add_argument(
        "--tts-model-id",
        type=str,
        default=None,
        help="TTS model ID for online loading (if different from model-path)"
    )
    parser.add_argument(
        "--quantization",
        type=str,
        default=None,
        choices=["int4", "int8", "awq-4bit"],
        help="Enable quantization for the TTS model to reduce memory usage."
             "Choices: int4 (online), int8 (online), awq-4bit (AWQ 4-bit quantization)."
             "When quantization is enabled, data types are handled automatically by the quantization library."
    )
    parser.add_argument(
        "--torch-dtype",
        type=str,
        default="bfloat16",
        choices=["float16", "bfloat16", "float32"],
        help="PyTorch data type for model operations. This setting only applies when quantization is disabled. "
             "When quantization is enabled, data types are managed automatically."
    )
    parser.add_argument(
        "--device-map",
        type=str,
        default="cuda",
        help="Device mapping for model loading (default: cuda)"
    )
    parser.add_argument(
        "--enable-auto-transcribe",
        action="store_true",
        help="Enable automatic audio transcription when uploading audio files (default: disabled)"
    )
    parser.add_argument(
        "--enable-api",
        action="store_true",
        help="Enable FastAPI endpoints (UI and API will share the same model instance)"
    )
    parser.add_argument(
        "--enable-gpu-management",
        action="store_true",
        default=True,
        help="Enable GPU memory management (lazy loading + auto offload). "
             "Models will be loaded on first use and offloaded to CPU after each task. (Default: enabled)"
    )
    parser.add_argument(
        "--gpu-idle-timeout",
        type=int,
        default=600,
        help="GPU idle timeout in seconds before auto-offloading to CPU (default: 600 = 10 minutes)"
    )

    args = parser.parse_args()

    # Map string arguments to actual types
    source_mapping = {
        "auto": ModelSource.AUTO,
        "local": ModelSource.LOCAL,
        "modelscope": ModelSource.MODELSCOPE,
        "huggingface": ModelSource.HUGGINGFACE
    }
    model_source = source_mapping[args.model_source]

    # Map torch dtype string to actual torch dtype
    dtype_mapping = {
        "float16": torch.float16,
        "bfloat16": torch.bfloat16,
        "float32": torch.float32
    }
    torch_dtype = dtype_mapping[args.torch_dtype]

    logger.info(f"Loading models with source: {args.model_source}")
    logger.info(f"Model path: {args.model_path}")
    logger.info(f"Tokenizer model ID: {args.tokenizer_model_id}")
    logger.info(f"Torch dtype: {args.torch_dtype}")
    logger.info(f"Device map: {args.device_map}")
    if args.tts_model_id:
        logger.info(f"TTS model ID: {args.tts_model_id}")
    if args.quantization:
        logger.info(f"ğŸ”§ {args.quantization.upper()} quantization enabled")

    # Initialize models
    whisper_asr = None
    try:
        # Load StepAudioTokenizer
        encoder = StepAudioTokenizer(
            os.path.join(args.model_path, "Step-Audio-Tokenizer"),
            model_source=model_source,
            funasr_model_id=args.tokenizer_model_id,
            enable_gpu_management=args.enable_gpu_management
        )
        logger.info("âœ“ StepAudioTokenizer loaded successfully")
        
        # Initialize TTS engine with optional GPU management
        tts_model_path = os.path.join(
            args.model_path, 
            "Step-Audio-EditX-AWQ-4bit" if args.quantization == "awq-4bit" else "Step-Audio-EditX"
        )
        
        if args.enable_gpu_management:
            logger.info(f"ğŸš€ GPU ç®¡ç†å·²å¯ç”¨ (è¶…æ—¶: {args.gpu_idle_timeout}ç§’)")
            common_tts_engine = GPUManagedTTS(
                model_path=tts_model_path,
                audio_tokenizer=encoder,
                model_source=model_source,
                tts_model_id=args.tts_model_id,
                quantization_config=args.quantization,
                torch_dtype=torch_dtype,
                device_map=args.device_map,
                gpu_idle_timeout=args.gpu_idle_timeout,
                enable_gpu_management=True
            )
        else:
            logger.info("â„¹ï¸  ä½¿ç”¨ä¼ ç»ŸåŠ è½½æ–¹å¼ï¼ˆGPU ç®¡ç†å·²ç¦ç”¨ï¼‰")
            common_tts_engine = StepAudioTTS(
                tts_model_path,
                encoder,
                model_source=model_source,
                tts_model_id=args.tts_model_id,
                quantization_config=args.quantization,
                torch_dtype=torch_dtype,
                device_map=args.device_map
            )
        logger.info("âœ“ StepCommonAudioTTS loaded successfully")
        
        # Prepare tts_engines dict for API (if enabled)
        tts_engines = {"base": common_tts_engine}
        
        if args.enable_auto_transcribe:
            whisper_asr = WhisperWrapper(enable_gpu_management=args.enable_gpu_management)
            logger.info("âœ“ Automatic audio transcription enabled")
    except Exception as e:
        logger.error(f"âŒ Error loading models: {e}")
        logger.error("Please check your model paths and source configuration.")
        exit(1)

    # Create EditxTab instance (pass encoder for cache stats and tts_engine for GPU management)
    editx_tab = EditxTab(args, encoder=encoder, tts_engine=common_tts_engine)

    # Launch demo with shared models
    launch_demo(args, editx_tab, encoder, tts_engines, whisper_asr)
